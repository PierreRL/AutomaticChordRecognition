\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Introduction}{1}{chapter.1}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Motivation}{1}{section.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1.2}Aims}{1}{section.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1.3}Outline}{1}{section.1.3}\protected@file@percent }
\citation{HarteNotation}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Background \& Related Work}{2}{chapter.2}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Background}{2}{section.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.1}Harmony, Chords and Chord Recognition}{2}{subsection.2.1.1}\protected@file@percent }
\citation{MusicGenChord}
\citation{Jukebox}
\citation{RAVE}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces An example of a lead sheet for `Yesterday' by the Beatles. We can see chords written above the stave and the melody written in standard musical notation. Such a chordal representation is useful for musicians who want to learn and perform songs quickly or improvise around them.}}{3}{figure.caption.2}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:lead_sheet_example}{{2.1}{3}{An example of a lead sheet for `Yesterday' by the Beatles. We can see chords written above the stave and the melody written in standard musical notation. Such a chordal representation is useful for musicians who want to learn and perform songs quickly or improvise around them}{figure.caption.2}{}}
\newlabel{fig:lead_sheet_example@cref}{{[figure][1][2]2.1}{[1][3][]3}{}{}{}}
\citation{ShazamSpectrogram}
\citation{PianoTranscriptionWithTransformer}
\citation{20YearsofACR}
\citation{CQT}
\citation{FirstDeepLearningCQT}
\citation{PianoTranscriptionWithTransformer}
\citation{BalanceRandomForestACR}
\citation{NNLSChroma}
\citation{HarmonyTransformer}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.2}Music Features}{4}{subsection.2.1.2}\protected@file@percent }
\newlabel{sec:background-features}{{2.1.2}{4}{Music Features}{subsection.2.1.2}{}}
\newlabel{sec:background-features@cref}{{[subsection][2][2,1]2.1.2}{[1][3][]4}{}{}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces A sample CQT of `Girls Just Wanna Have Fun' by Cyndi Lauper from the dataset used in this work. We can see the log-spaced frequency bins on the y-axis. There is clear structure and repetition in the song, particularly in the lower frequencies, which can be attributed to a regular drum groove and bass instruments. This is typical of songs in this dataset. Such structure and repetition gives an idea of the patterns a machine learning model may look for to identify chords.}}{4}{figure.caption.3}\protected@file@percent }
\newlabel{fig:cqt_example}{{2.2}{4}{A sample CQT of `Girls Just Wanna Have Fun' by Cyndi Lauper from the dataset used in this work. We can see the log-spaced frequency bins on the y-axis. There is clear structure and repetition in the song, particularly in the lower frequencies, which can be attributed to a regular drum groove and bass instruments. This is typical of songs in this dataset. Such structure and repetition gives an idea of the patterns a machine learning model may look for to identify chords}{figure.caption.3}{}}
\newlabel{fig:cqt_example@cref}{{[figure][2][2]2.2}{[1][4][]4}{}{}{}}
\citation{MelodyTranscriptionViaGenerativePreTraining}
\citation{Jukebox}
\citation{AttentionIsAllYouNeed}
\citation{20YearsofACR}
\citation{FujishimaACR}
\citation{McgillBillboard}
\citation{Isophonics}
\citation{RWC}
\citation{USPop}
\citation{JAAH}
\citation{MelodyTranscriptionViaGenerativePreTraining}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Related Work}{5}{section.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.1}Automatic Chord Recognition}{5}{subsection.2.2.1}\protected@file@percent }
\newlabel{sec:background-acr}{{2.2.1}{5}{Automatic Chord Recognition}{subsection.2.2.1}{}}
\newlabel{sec:background-acr@cref}{{[subsection][1][2,2]2.2.1}{[1][5][]5}{}{}{}}
\citation{Choco}
\citation{ScalingUpSemiSupervisedLearning}
\citation{MusicGenTrainingData,AnnotationFreeSyntheticData}
\citation{MERTSupervisedLearning}
\citation{ACRLargeVocab1}
\citation{StructuredTraining,ACRLargeVocab1}
\citation{BalanceRandomForestACR}
\citation{CurriculumLearning}
\citation{ACRHMM}
\citation{ACRCNNRNN1,ACRLargeVocab1,StructuredTraining}
\citation{MelodyTranscriptionViaGenerativePreTraining,HarmonyTransformer,AttendToChords}
\citation{BalanceRandomForestACR}
\citation{ACRLargeVocab1}
\citation{mir_eval}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.2}Synthetic Data Generation}{6}{subsection.2.2.2}\protected@file@percent }
\citation{Choco}
\citation{librosa}
\citation{StructuredTraining}
\citation{MelodyTranscriptionViaGenerativePreTraining}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Experimental Setup}{7}{chapter.3}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Data}{7}{section.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.1}Preprocessing}{7}{subsection.3.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.1.1}Audio to CQT}{7}{subsubsection.3.1.1.1}\protected@file@percent }
\newlabel{sec:audio-to-cqt}{{3.1.1.1}{7}{Audio to CQT}{subsubsection.3.1.1.1}{}}
\newlabel{sec:audio-to-cqt@cref}{{[subsubsection][1][3,1,1]3.1.1.1}{[1][7][]7}{}{}{}}
\citation{HarteNotation}
\citation{StructuredTraining,FourTimelyInsights,ACRLargeVocab1}
\citation{StructuredTraining}
\citation{music21}
\citation{StructuredTraining}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.1.2}Chord Annotations}{8}{subsubsection.3.1.1.2}\protected@file@percent }
\newlabel{sec:chord-annotations}{{3.1.1.2}{8}{Chord Annotations}{subsubsection.3.1.1.2}{}}
\newlabel{sec:chord-annotations@cref}{{[subsubsection][2][3,1,1]3.1.1.2}{[1][8][]8}{}{}{}}
\citation{FourTimelyInsights}
\citation{JAMS}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.2}Pop Dataset}{9}{subsection.3.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.2.1}Data Integrity}{9}{subsubsection.3.1.2.1}\protected@file@percent }
\newlabel{sec:data-integrity}{{3.1.2.1}{9}{Data Integrity}{subsubsection.3.1.2.1}{}}
\newlabel{sec:data-integrity@cref}{{[subsubsection][1][3,1,2]3.1.2.1}{[1][9][]9}{}{}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces Cross-correlation of the derivative of the CQT of the audio and the chord annotations for a single song. We can see correlation peaking in regular intervals of around 20 frames, which corresponds to one bar length in this song. Zooming out, we observe peaks in correlation centred around 0.}}{10}{figure.caption.4}\protected@file@percent }
\newlabel{fig:cross-correlation}{{3.1}{10}{Cross-correlation of the derivative of the CQT of the audio and the chord annotations for a single song. We can see correlation peaking in regular intervals of around 20 frames, which corresponds to one bar length in this song. Zooming out, we observe peaks in correlation centred around 0}{figure.caption.4}{}}
\newlabel{fig:cross-correlation@cref}{{[figure][1][3]3.1}{[1][9][]10}{}{}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces Cross-correlation of the derivative of the CQT of the audio and the chord annotations for a single song. The x-axis is the lag in frames and the y-axis is the correlation. The plot repeats every 100 frames, which corresponds to 4 bars.}}{10}{figure.caption.5}\protected@file@percent }
\newlabel{fig:durations-and-lags}{{3.2}{10}{Cross-correlation of the derivative of the CQT of the audio and the chord annotations for a single song. The x-axis is the lag in frames and the y-axis is the correlation. The plot repeats every 100 frames, which corresponds to 4 bars}{figure.caption.5}{}}
\newlabel{fig:durations-and-lags@cref}{{[figure][2][3]3.2}{[1][10][]10}{}{}{}}
\citation{FourTimelyInsights}
\citation{StructuredTraining}
\citation{BalanceRandomForestACR,CurriculumLearning}
\citation{ACRLargeVocab1}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.1.2.2}Chord Distribution}{11}{subsubsection.3.1.2.2}\protected@file@percent }
\citation{mir_eval}
\citation{FourTimelyInsights}
\@writefile{lof}{\contentsline {figure}{\numberline {3.3}{\ignorespaces Chord distributions in the \emph  {Pop} dataset. The plots show both the raw counts in terms of frames and the duration in seconds for each chord root/quality. Note that the y-axis on qualities is in log-scale. We observe that the qualities are very imbalanced, with \texttt  {maj} the most popular, but that roots are relatively balanced.}}{12}{figure.caption.6}\protected@file@percent }
\newlabel{fig:chord-distribution}{{3.3}{12}{Chord distributions in the \emph {Pop} dataset. The plots show both the raw counts in terms of frames and the duration in seconds for each chord root/quality. Note that the y-axis on qualities is in log-scale. We observe that the qualities are very imbalanced, with \texttt {maj} the most popular, but that roots are relatively balanced}{figure.caption.6}{}}
\newlabel{fig:chord-distribution@cref}{{[figure][3][3]3.3}{[1][11][]12}{}{}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.3}JAAH Dataset}{12}{subsection.3.1.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Evaluation}{12}{section.3.2}\protected@file@percent }
\newlabel{sec:evaluation}{{3.2}{12}{Evaluation}{section.3.2}{}}
\newlabel{sec:evaluation@cref}{{[section][2][3]3.2}{[1][12][]12}{}{}{}}
\citation{pytorch}
\citation{adam}
\citation{FourTimelyInsights}
\@writefile{toc}{\contentsline {section}{\numberline {3.3}Training}{14}{section.3.3}\protected@file@percent }
\newlabel{sec:training}{{3.3}{14}{Training}{section.3.3}{}}
\newlabel{sec:training@cref}{{[section][3][3]3.3}{[1][14][]14}{}{}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Model Comparison}{15}{chapter.4}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Logistic Baseline}{15}{section.4.1}\protected@file@percent }
\citation{StructuredTraining}
\citation{GRU}
\citation{StructuredTraining}
\@writefile{lot}{\contentsline {table}{\numberline {4.1}{\ignorespaces Baseline model results}}{16}{table.caption.7}\protected@file@percent }
\newlabel{tab:baseline_results}{{4.1}{16}{Baseline model results}{table.caption.7}{}}
\newlabel{tab:baseline_results@cref}{{[table][1][4]4.1}{[1][16][]16}{}{}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.2}CRNN}{16}{section.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.1}Model Description}{16}{subsection.4.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.2}Small to Large Vocabulary}{16}{subsection.4.2.2}\protected@file@percent }
\citation{BTC}
\@writefile{lot}{\contentsline {table}{\numberline {4.2}{\ignorespaces CRNN model results on the small vocabulary with $V=26$. The other metrics are omitted as they are identical to \texttt  {third} for classification with $V=26$.}}{17}{table.caption.8}\protected@file@percent }
\newlabel{tab:crnn_small_vocab}{{4.2}{17}{CRNN model results on the small vocabulary with $V=26$. The other metrics are omitted as they are identical to \texttt {third} for classification with $V=26$}{table.caption.8}{}}
\newlabel{tab:crnn_small_vocab@cref}{{[table][2][4]4.2}{[1][17][]17}{}{}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.1}{\ignorespaces Confusion matrix over roots of the CRNN model trained on the small vocabulary. The values have been normalised over rows such that the values on the diagonals are recall metrics. There is a clear outlier in the model's recall with the label \texttt  {X}, at just $0.07$. It also performs poorly on \texttt  {N}.}}{18}{figure.caption.9}\protected@file@percent }
\newlabel{fig:crnn_small_vocab_cm}{{4.1}{18}{Confusion matrix over roots of the CRNN model trained on the small vocabulary. The values have been normalised over rows such that the values on the diagonals are recall metrics. There is a clear outlier in the model's recall with the label \texttt {X}, at just $0.07$. It also performs poorly on \texttt {N}}{figure.caption.9}{}}
\newlabel{fig:crnn_small_vocab_cm@cref}{{[figure][1][4]4.1}{[1][17][]18}{}{}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.3}Hyperparameter Tuning}{18}{subsection.4.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.3.1}Learning rates}{18}{subsubsection.4.2.3.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.2}{\ignorespaces Training graphs for the CRNN model with different learning rates. The learning rate of $0.001$ seems to be the best, as it converges in a reasonable time and the validation accuracy increases in a stable fashion.}}{19}{figure.caption.10}\protected@file@percent }
\newlabel{fig:lr_search_cosine}{{4.2}{19}{Training graphs for the CRNN model with different learning rates. The learning rate of $0.001$ seems to be the best, as it converges in a reasonable time and the validation accuracy increases in a stable fashion}{figure.caption.10}{}}
\newlabel{fig:lr_search_cosine@cref}{{[figure][2][4]4.2}{[1][19][]19}{}{}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4.3}{\ignorespaces CRNN model results on the large vocabulary with different learning rates and schedulers. Overall, a learning rate of $0.001$ and a scheduler of \texttt  {Cosine} performs the best in many metrics, though a scheduler of \texttt  {Plateau} performs just as well or better on many metrics. We prioritise the performance of the model on the root as this is more important than the \texttt  {mirex} metric. }}{19}{table.caption.11}\protected@file@percent }
\newlabel{tab:crnn_lr}{{4.3}{19}{CRNN model results on the large vocabulary with different learning rates and schedulers. Overall, a learning rate of $0.001$ and a scheduler of \texttt {Cosine} performs the best in many metrics, though a scheduler of \texttt {Plateau} performs just as well or better on many metrics. We prioritise the performance of the model on the root as this is more important than the \texttt {mirex} metric}{table.caption.11}{}}
\newlabel{tab:crnn_lr@cref}{{[table][3][4]4.3}{[1][19][]19}{}{}{}}
\citation{StructuredTraining}
\citation{ACRLargeVocab1}
\citation{CurriculumLearning}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.3.2}Model Hyperparameters}{20}{subsubsection.4.2.3.2}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {4.4}{\ignorespaces CRNN model results on the large vocabulary with different hyperparameters. Best unrounded metrics are bolded. Results across all hyperparameters were very similar. We proceed with the best model found with hidden size of $201$, a single layer GRU and a segment length of $28$, although any configuration would work similarly. This may implies that model is learning something simple because the increased complexity of larger models did not help with performance. The variance within the data may be entirely due to the stochastic nature of SGD. Regardless of what makes the results different, the effect size is small enough to conclude that it does not make a meaningful difference.}}{20}{table.caption.12}\protected@file@percent }
\newlabel{tab:crnn_hparams}{{4.4}{20}{CRNN model results on the large vocabulary with different hyperparameters. Best unrounded metrics are bolded. Results across all hyperparameters were very similar. We proceed with the best model found with hidden size of $201$, a single layer GRU and a segment length of $28$, although any configuration would work similarly. This may implies that model is learning something simple because the increased complexity of larger models did not help with performance. The variance within the data may be entirely due to the stochastic nature of SGD. Regardless of what makes the results different, the effect size is small enough to conclude that it does not make a meaningful difference}{table.caption.12}{}}
\newlabel{tab:crnn_hparams@cref}{{[table][4][4]4.4}{[1][20][]20}{}{}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.4}Model Analysis}{21}{subsection.4.2.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.4.1}Qualities and Roots}{21}{subsubsection.4.2.4.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.3}{\ignorespaces Row-normalised confusion matrices over qualities of the \emph  {CRNN} model without (above) and with (below) weighted loss. The weighting is with with $\alpha = 0.55$ as in Equation\ref {eq:weighted_loss}. Rows are ordered by frequency of chord quality. We can see that both models struggle with the long tail. However, weighting the loss does improve the model, notably on \texttt  {7} and \texttt  {maj7} qualities and predicts \texttt  {maj} less often. Recall on the \texttt  {maj} worsens by $0.18$ and recall on \texttt  {X} decreases from $0.24$ to $0.05$. The weighted model predicts \texttt  {X} approximately four times less often. This may be how the weighted model improves class-wise metrics without sacrificing too much overall accuracy, since \texttt  {X} frames are ignored. We can also see that both models frequently confuse \texttt  {dim7} and \texttt  {dim} qualities, consistently predict \texttt  {maj} for \texttt  {sus2, sus4, maj6, maj7} and \texttt  {minmaj7} and struggles with \texttt  {aug}.}}{22}{figure.caption.13}\protected@file@percent }
\newlabel{fig:crnn_qual_cm}{{4.3}{22}{Row-normalised confusion matrices over qualities of the \emph {CRNN} model without (above) and with (below) weighted loss. The weighting is with with $\alpha = 0.55$ as in Equation\ref {eq:weighted_loss}. Rows are ordered by frequency of chord quality. We can see that both models struggle with the long tail. However, weighting the loss does improve the model, notably on \texttt {7} and \texttt {maj7} qualities and predicts \texttt {maj} less often. Recall on the \texttt {maj} worsens by $0.18$ and recall on \texttt {X} decreases from $0.24$ to $0.05$. The weighted model predicts \texttt {X} approximately four times less often. This may be how the weighted model improves class-wise metrics without sacrificing too much overall accuracy, since \texttt {X} frames are ignored. We can also see that both models frequently confuse \texttt {dim7} and \texttt {dim} qualities, consistently predict \texttt {maj} for \texttt {sus2, sus4, maj6, maj7} and \texttt {minmaj7} and struggles with \texttt {aug}}{figure.caption.13}{}}
\newlabel{fig:crnn_qual_cm@cref}{{[figure][3][4]4.3}{[1][21][]22}{}{}{}}
\citation{HarmonyTransformer}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.4.2}Transition Frames}{23}{subsubsection.4.2.4.2}\protected@file@percent }
\newlabel{sec:transition_frames}{{4.2.4.2}{23}{Transition Frames}{subsubsection.4.2.4.2}{}}
\newlabel{sec:transition_frames@cref}{{[subsubsection][2][4,2,4]4.2.4.2}{[1][23][]23}{}{}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.4.3}Smoothness}{23}{subsubsection.4.2.4.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.4.4}Performance Across the Context}{24}{subsubsection.4.2.4.4}\protected@file@percent }
\newlabel{sec:crnn_performance_across_context}{{4.2.4.4}{24}{Performance Across the Context}{subsubsection.4.2.4.4}{}}
\newlabel{sec:crnn_performance_across_context@cref}{{[subsubsection][4][4,2,4]4.2.4.4}{[1][23][]24}{}{}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.4}{\ignorespaces Average frame-wise accuracy of the \emph  {CRNN} model over the patch of audio. The model performs worse at the beginning and end of the patch of audio, as expected. However, the differences are only $~0.05$. We propose that the context on one side is enough for the model to attain the vast majority of the performance attained with bi-directional context. This plot supports our procedure of evaluating over the entire song at once. }}{24}{figure.caption.14}\protected@file@percent }
\newlabel{fig:crnn_context}{{4.4}{24}{Average frame-wise accuracy of the \emph {CRNN} model over the patch of audio. The model performs worse at the beginning and end of the patch of audio, as expected. However, the differences are only $~0.05$. We propose that the context on one side is enough for the model to attain the vast majority of the performance attained with bi-directional context. This plot supports our procedure of evaluating over the entire song at once}{figure.caption.14}{}}
\newlabel{fig:crnn_context@cref}{{[figure][4][4]4.4}{[1][24][]24}{}{}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.4.5}Generalising Across Songs}{24}{subsubsection.4.2.4.5}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.5}{\ignorespaces Histogram of accuracies and mirex scores over songs in the validation set. Accuracies are mixed, with 17\% of songs below $0.4$, and 64\% between $0.4$ and $0.8$. However, with the more generous \texttt  {mirex} metric, we find that there are almost no songs below $0.4$ and only 6\% below $0.6$. Many of the mistakes the model makes are small, like predicting \texttt  {C:maj} instead of \texttt  {C:maj7}. Such examples are discussed in more detail in Section~\ref {sec:crnn_examples}. The very low outliers in the \texttt  {mirex} score were found to be songs with incorrect annotations found in Section~\ref {sec:data-integrity}.}}{25}{figure.caption.15}\protected@file@percent }
\newlabel{fig:crnn_song_hist}{{4.5}{25}{Histogram of accuracies and mirex scores over songs in the validation set. Accuracies are mixed, with 17\% of songs below $0.4$, and 64\% between $0.4$ and $0.8$. However, with the more generous \texttt {mirex} metric, we find that there are almost no songs below $0.4$ and only 6\% below $0.6$. Many of the mistakes the model makes are small, like predicting \texttt {C:maj} instead of \texttt {C:maj7}. Such examples are discussed in more detail in Section~\ref {sec:crnn_examples}. The very low outliers in the \texttt {mirex} score were found to be songs with incorrect annotations found in Section~\ref {sec:data-integrity}}{figure.caption.15}{}}
\newlabel{fig:crnn_song_hist@cref}{{[figure][5][4]4.5}{[1][25][]25}{}{}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.4.6}Four Illustrative Examples}{25}{subsubsection.4.2.4.6}\protected@file@percent }
\newlabel{sec:crnn_examples}{{4.2.4.6}{25}{Four Illustrative Examples}{subsubsection.4.2.4.6}{}}
\newlabel{sec:crnn_examples@cref}{{[subsubsection][6][4,2,4]4.2.4.6}{[1][25][]25}{}{}{}}
\citation{CurriculumLearning}
\citation{BalanceRandomForestACR}
\citation{ACRLargeVocab1}
\@writefile{lof}{\contentsline {figure}{\numberline {4.6}{\ignorespaces Chord predictions of the \emph  {CRNN} model on four songs from the validation set (blue: correct, red: incorrect, gray: \texttt  {X}). This allows us to understand some of the behaviour of the model. We can see regular repeated errors in `Mr.\ Moonlight', which are mostly mistaking two similar qualities. The discrepancy between accuracy and \texttt  {mirex} on `Ain't No Sunshine' can be explained by missing sevenths in many predictions. The large incorrect region is a voice and drum only section where the model continues to predict chords due to implied harmony by the melody. Predictions in `Brandy' are quite good in general, though many errors arise from predicting the boundaries of chord changes incorrectly. The model struggles with `Earth, Wind and Fire', missing chord boundaries, and sometimes predicting completely wrong chords. There are clearly songs where the model's outputs are less sensible. However, in general most of the model's mistakes can be explained and are reasonable.}}{27}{figure.caption.16}\protected@file@percent }
\newlabel{fig:crnn_examples}{{4.6}{27}{Chord predictions of the \emph {CRNN} model on four songs from the validation set (blue: correct, red: incorrect, gray: \texttt {X}). This allows us to understand some of the behaviour of the model. We can see regular repeated errors in `Mr.\ Moonlight', which are mostly mistaking two similar qualities. The discrepancy between accuracy and \texttt {mirex} on `Ain't No Sunshine' can be explained by missing sevenths in many predictions. The large incorrect region is a voice and drum only section where the model continues to predict chords due to implied harmony by the melody. Predictions in `Brandy' are quite good in general, though many errors arise from predicting the boundaries of chord changes incorrectly. The model struggles with `Earth, Wind and Fire', missing chord boundaries, and sometimes predicting completely wrong chords. There are clearly songs where the model's outputs are less sensible. However, in general most of the model's mistakes can be explained and are reasonable}{figure.caption.16}{}}
\newlabel{fig:crnn_examples@cref}{{[figure][6][4]4.6}{[1][26][]27}{}{}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.3}Weighted Loss}{27}{section.4.3}\protected@file@percent }
\citation{ACRLargeVocab1}
\newlabel{eq:weighted_loss}{{4.2}{28}{Weighted Loss}{equation.4.3.2}{}}
\newlabel{eq:weighted_loss@cref}{{[equation][2][4]4.2}{[1][28][]28}{}{}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.7}{\ignorespaces Effect of weighted loss on the \emph  {CRNN} model with varying $\alpha $. As we increase $\alpha $, class-wise metrics improve but accuracy-based metrics worsen. We claim a sweet-spot in the middle where we trade only a little overall performance for better class-wise recall. We choose this to be $\alpha = 0.55$. The \texttt  {root} and \texttt  {third} metrics improve and less than $3\%$ is lost on other metrics while mean class-wise accuracy improves by $6\%$ and the median improved by $0.2$. This plot also reveals strong correlation between metrics. }}{28}{figure.caption.17}\protected@file@percent }
\newlabel{fig:weighted_loss}{{4.7}{28}{Effect of weighted loss on the \emph {CRNN} model with varying $\alpha $. As we increase $\alpha $, class-wise metrics improve but accuracy-based metrics worsen. We claim a sweet-spot in the middle where we trade only a little overall performance for better class-wise recall. We choose this to be $\alpha = 0.55$. The \texttt {root} and \texttt {third} metrics improve and less than $3\%$ is lost on other metrics while mean class-wise accuracy improves by $6\%$ and the median improved by $0.2$. This plot also reveals strong correlation between metrics}{figure.caption.17}{}}
\newlabel{fig:weighted_loss@cref}{{[figure][7][4]4.7}{[1][28][]28}{}{}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.4}Pitch Augmentation}{28}{section.4.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4.5}Structured Training}{29}{section.4.5}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4.6}Transformer}{29}{section.4.6}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4.7}Using Generative Features}{29}{section.4.7}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4.8}Decoding}{29}{section.4.8}\protected@file@percent }
\newlabel{sec:decoding}{{4.8}{29}{Decoding}{section.4.8}{}}
\newlabel{sec:decoding@cref}{{[section][8][4]4.8}{[1][29][]29}{}{}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.9}Results on Test Set}{29}{section.4.9}\protected@file@percent }
\@writefile{toc}{\contentsline {chapter}{\numberline {5}Synthetic Data Generation}{30}{chapter.5}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chap:synthetic_data}{{5}{30}{Synthetic Data Generation}{chapter.5}{}}
\newlabel{chap:synthetic_data@cref}{{[chapter][5][]5}{[1][30][]30}{}{}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.1}Performance on JAAH}{30}{section.5.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5.2}Generation method}{30}{section.5.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5.3}Experiments}{30}{section.5.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5.4}Results}{30}{section.5.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5.5}Qualitative Analysis}{30}{section.5.5}\protected@file@percent }
\bibstyle{plainnat}
\bibdata{references}
\@writefile{toc}{\contentsline {chapter}{\numberline {6}Conclusions, Limitations and Further Work}{31}{chapter.6}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {6.1}Conclusions}{31}{section.6.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {6.2}Limitations}{31}{section.6.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {6.3}Further Work}{31}{section.6.3}\protected@file@percent }
\bibcite{USPop}{{1}{2004}{{Berenzweig et~al.}}{{Berenzweig, Logan, Ellis, and Whitman}}}
\bibcite{CQT}{{2}{1991}{{Brown}}{{}}}
\bibcite{McgillBillboard}{{3}{2011}{{Burgoyne et~al.}}{{Burgoyne, Wild, and Fujinaga}}}
\bibcite{RAVE}{{4}{2021}{{Caillon and Esling}}{{}}}
\bibcite{Isophonics}{{5}{2009}{{Cannam et~al.}}{{Cannam, Landone, and Sandler}}}
\bibcite{HarmonyTransformer}{{6}{2019}{{Chen and Su}}{{}}}
\bibcite{AttendToChords}{{7}{2021}{{Chen and Su}}{{}}}
\bibcite{GRU}{{8}{2014}{{Cho et~al.}}{{Cho, van Merri{\"e}nboer, Gulcehre, Bahdanau, Bougares, Schwenk, and Bengio}}}
\bibcite{MelodyTranscriptionViaGenerativePreTraining}{{9}{2022}{{Chris~Donahue and Liang}}{{}}}
\bibcite{music21}{{10}{2010}{{Cuthbert and Ariza}}{{}}}
\@writefile{toc}{\contentsline {chapter}{Bibliography}{32}{chapter*.18}\protected@file@percent }
\bibcite{Choco}{{11}{2023}{{de~Berardinis et~al.}}{{de~Berardinis, Meroño-Peñuela, Poltronieri, and Presutti}}}
\bibcite{Jukebox}{{12}{2020}{{Dhariwal et~al.}}{{Dhariwal, Jun, Payne, Kim, Radford, and Sutskever}}}
\bibcite{JAAH}{{13}{2020}{{Durán and de~la Cuadra}}{{}}}
\bibcite{FujishimaACR}{{14}{1999}{{Fujishima}}{{}}}
\bibcite{RWC}{{15}{2002}{{Goto et~al.}}{{Goto, Hashiguchi, Nishimura, and Oka}}}
\bibcite{HarteNotation}{{16}{2005}{{Harte et~al.}}{{Harte, Sandler, Abdallah, and Gómez}}}
\bibcite{FirstDeepLearningCQT}{{17}{2012}{{Humphrey and Bello}}{{}}}
\bibcite{FourTimelyInsights}{{18}{2015}{{Humphrey and Bello}}{{}}}
\bibcite{JAMS}{{19}{2014}{{Humphrey et~al.}}{{Humphrey, Salamon, Nieto, Forsyth, Bittner, and Bello}}}
\bibcite{ScalingUpSemiSupervisedLearning}{{20}{2023}{{Hung et~al.}}{{Hung, Wang, Won, and Le}}}
\bibcite{ACRLargeVocab1}{{21}{2019}{{Jiang et~al.}}{{Jiang, Chen, Li, and Xia}}}
\bibcite{MusicGenChord}{{22}{2024}{{Jung et~al.}}{{Jung, Jansson, and Jeong}}}
\bibcite{adam}{{23}{2015}{{Kingma and Ba}}{{}}}
\bibcite{MusicGenTrainingData}{{24}{2023}{{Kroher et~al.}}{{Kroher, Cuesta, and Pikrakis}}}
\bibcite{ACRHMM}{{25}{2006}{{Lee and Slaney}}{{}}}
\bibcite{MERTSupervisedLearning}{{26}{2024}{{Li et~al.}}{{Li, Yuan, Zhang, Ma, Chen, Yin, Xiao, Lin, Ragni, Benetos, Gyenge, Dannenberg, Liu, Chen, Xia, Shi, Huang, Wang, Guo, and Fu}}}
\bibcite{NNLSChroma}{{27}{2010}{{Mauch and Dixon}}{{}}}
\bibcite{StructuredTraining}{{28}{2017}{{McFee and Bello}}{{}}}
\bibcite{librosa}{{29}{2015}{{McFee et~al.}}{{McFee, Raffel, Liang, Ellis, Mcvicar, Battenberg, and Nieto}}}
\bibcite{BalanceRandomForestACR}{{30}{2022}{{Miller et~al.}}{{Miller, O'Hanlon, and Sandler}}}
\bibcite{BTC}{{31}{2019}{{Park et~al.}}{{Park, Choi, Jeon, Kim, and Park}}}
\bibcite{pytorch}{{32}{2019}{{Paszke et~al.}}{{Paszke, Gross, Massa, Lerer, Bradbury, Chanan, Killeen, Lin, Gimelshein, Antiga, Desmaison, K{\"o}pf, Yang, DeVito, Raison, Tejani, Chilamkurthy, Steiner, Fang, Bai, and Chintala}}}
\bibcite{20YearsofACR}{{33}{2019}{{Pauwels et~al.}}{{Pauwels, O'Hanlon, G{\'o}mez, and Sandler}}}
\bibcite{mir_eval}{{34}{2014}{{Raffel et~al.}}{{Raffel, McFee, Humphrey, Salamon, Nieto, Liang, and Ellis}}}
\bibcite{CurriculumLearning}{{35}{2021}{{Rowe and Tzanetakis}}{{}}}
\bibcite{AnnotationFreeSyntheticData}{{36}{2024}{{Sato and Akama}}{{}}}
\bibcite{PianoTranscriptionWithTransformer}{{37}{2023}{{Toyama et~al.}}{{Toyama, Akama, Ikemiya, Takida, Liao, and Mitsufuji}}}
\bibcite{AttentionIsAllYouNeed}{{38}{2023}{{Vaswani et~al.}}{{Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez, Kaiser, and Polosukhin}}}
\bibcite{ShazamSpectrogram}{{39}{2003}{{Wang}}{{}}}
\bibcite{ACRCNNRNN1}{{40}{2019}{{Wu et~al.}}{{Wu, Carsault, and Yoshii}}}
\@writefile{toc}{\contentsline {chapter}{\numberline {A}Appendix}{36}{appendix.A}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {A.1}Histogram over Incorrect Region Lengths}{36}{section.A.1}\protected@file@percent }
\newlabel{app:histogram_over_region_lengths}{{A.1}{36}{Histogram over Incorrect Region Lengths}{section.A.1}{}}
\newlabel{app:histogram_over_region_lengths@cref}{{[subappendix][1][2147483647,1]A.1}{[1][36][]36}{}{}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {A.1}{\ignorespaces Histogram over incorrect region lengths. The incorrect region distribution has a long-tail, with $26.7\%$ regions being of length 1. This raises concerns over the smoothness of outputs and requires some form of post-processing explored in Section~\ref {sec:decoding}.}}{36}{figure.caption.19}\protected@file@percent }
\newlabel{fig:histogram_over_region_lengths}{{A.1}{36}{Histogram over incorrect region lengths. The incorrect region distribution has a long-tail, with $26.7\%$ regions being of length 1. This raises concerns over the smoothness of outputs and requires some form of post-processing explored in Section~\ref {sec:decoding}}{figure.caption.19}{}}
\newlabel{fig:histogram_over_region_lengths@cref}{{[figure][1][2147483647,1]A.1}{[1][36][]36}{}{}{}}
\@writefile{toc}{\contentsline {section}{\numberline {A.2}Accuracy vs Context Length of Evaluation}{37}{section.A.2}\protected@file@percent }
\newlabel{app:accuracy_vs_context_length}{{A.2}{37}{Accuracy vs Context Length of Evaluation}{section.A.2}{}}
\newlabel{app:accuracy_vs_context_length@cref}{{[subappendix][2][2147483647,1]A.2}{[1][37][]37}{}{}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {A.2}{\ignorespaces Accuracy vs context length of evaluation. The accuracy increases very slightly. The effect size is so small that we conclude it does not make a difference, and choose to evaluate over the entire song at once.}}{37}{figure.caption.20}\protected@file@percent }
\newlabel{fig:accuracy_vs_context_length}{{A.2}{37}{Accuracy vs context length of evaluation. The accuracy increases very slightly. The effect size is so small that we conclude it does not make a difference, and choose to evaluate over the entire song at once}{figure.caption.20}{}}
\newlabel{fig:accuracy_vs_context_length@cref}{{[figure][2][2147483647,1]A.2}{[1][37][]37}{}{}{}}
\@writefile{toc}{\contentsline {section}{\numberline {A.3}Accuracy vs Hop Length}{37}{section.A.3}\protected@file@percent }
\newlabel{app:accuracy_vs_hop_length}{{A.3}{37}{Accuracy vs Hop Length}{section.A.3}{}}
\newlabel{app:accuracy_vs_hop_length@cref}{{[subappendix][3][2147483647,1]A.3}{[1][37][]37}{}{}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {A.3}{\ignorespaces Accuracy vs hop length. Metrics are not directly comparable over hop lengths due to different likelihoods. However, the metrics are fairly consistent over different hop lengths, certainly over the region explored by the literature $[512,2048,4096]$. Every hop length tested is short enough to be more granular than chords, but not so short that the computed CQT is too noisy. We continue with the default hop length of $4096$, to be consistent with some of the literature while keeping computational cost low.}}{37}{figure.caption.21}\protected@file@percent }
\newlabel{fig:accuracy_vs_hop_length}{{A.3}{37}{Accuracy vs hop length. Metrics are not directly comparable over hop lengths due to different likelihoods. However, the metrics are fairly consistent over different hop lengths, certainly over the region explored by the literature $[512,2048,4096]$. Every hop length tested is short enough to be more granular than chords, but not so short that the computed CQT is too noisy. We continue with the default hop length of $4096$, to be consistent with some of the literature while keeping computational cost low}{figure.caption.21}{}}
\newlabel{fig:accuracy_vs_hop_length@cref}{{[figure][3][2147483647,1]A.3}{[1][37][]37}{}{}{}}
\gdef \@abspage@last{43}
