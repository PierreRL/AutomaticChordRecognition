\appendix

\chapter{Appendix}

\section{Limitations and Ethics Statement}\label{app:limitations_and_ethics}

It is worth bearing in mind the limitations of the work presented here and its conclusions. The dataset used focuses largely on pop and rock. Cross-genre generalisation is not considered. These models expect music in standard tuning in the Wester chromatic scale. Chords themselves are also a highly Western concept. Not all music can be well described by the harmony structures considered in the chord vocabulary in this work. 

There are also ethical issues relevant to training musical machine learning models worth mentioning. The audio data in this work is subject to copyright. Research on chord recognition falls under fair use but distribution of such data should be carefully controlled. 

The generative models also present an ethical dilemma. Such models can be trained on copyrighted data without proper legal agreements with rights holder. The authors of the MusicGen models used in this work claim to have addressed such concerns. I do not support the use of work in chord recognition as a basis for advancing music generation models without proper legal and ethical issues being addressed.

\section{Weighted Chord Symbol Recall Definitions}\label{app:weighted_chord_symbol_recall_definitions}

A formal definition of WCSR is provided in Equation~\ref{eq:wcsr}. The WCSR is a measure of the percentage of time that the model's predictions are correct.

\begin{equation}\label{eq:wcsr}
    WCSR = 100\cdot\frac{1}{Z}\sum_{i=1}^{N} \int_{t=0}^{T_i} M(y_{i,t},\hat{y}_{i,t}) dt
\end{equation}
\begin{equation}
    Z = \sum_{i=1}^{N} \int_{t=0}^{T_i} \mathbb{I}_M(y_{i,t}) dt
\end{equation}

where $M(y, \hat{y})\in\{0,1\}$ is the measure of correctness which varies across metrics. For example, $M(y, \hat{y})$ for \texttt{root} equals $1$ if $y$ and $\hat{y}$ share the same root and $0$ otherwise. $N$ is the number of songs, $T_i$ is the length of song $i$, $y_{i,t}$ is the true chord at time $t$ of song $i$, and $\hat{y}_{i,t}$ is the predicted chord at time $t$ of song $i$. $Z$ normalises by the length of time for which the metric $M$ is defined. This is necessary as \texttt{X} symbols are ignored and \texttt{seventh} ignores some qualities. Further details can be found in the \texttt{mir\_eval} documentation. $\mathbb{I}_M(y_{i,t})=1$ if $M$ is defined for label $y_{i,t}$ and $0$ otherwise. Finally, we multiply by 100 to convert to a percentage.

I also define WCSR for a single class $c$ in Equation~\ref{eq:wcsr_c}. This is useful for understanding the performance of the model on a specific chord class.

\begin{equation}\label{eq:wcsr_c}
    \text{WCSR}(c) = \frac{1}{Z_c}\sum_{i=1}^{N} \int_{t=0}^{T_i} M(y_{i,t},\hat{y}_{i,t}) \cdot \mathbb{I}_c(y_{i,t}) dt
\end{equation}
\begin{equation}
    Z_c = \sum_{i=1}^{N} \int_{t=0}^{T_i} \mathbb{I}_M(y_{i,t})\cdot \mathbb{I}_c(y_{i,t}) dt
\end{equation}

where $N$, $T$, $M$, $y_{i,t}$, $\hat{y}_{i,t}$ and $\mathbb{I}_M(y_{i,t})$ are defined as before in Equation~\ref{eq:wcsr}. $\mathbb{I}_c(y_{i,t})$ is $1$ if the true chord at time $t$ of song $i$ is class $c$ abd $0$ otherwise. $Z_c$ normalises by the length of time for which the chord $c$ is playing and for which the metric $M$ is defined, in a similar fashion to $Z$ in Equation~\ref{eq:wcsr}.

\section{Cross Correlation for Alignment Verification}\label{app:cross_correlation}

\begin{figure}[H]
    \centering
    \includegraphics[width=1.0\textwidth]{figures/cross_correlation.png}
    \caption{Cross-correlation of the derivative of the CQT of the audio and the chord annotations for a single song. We can see correlation peaking in regular intervals of around 20 frames. 1 frame is $93$ms so 20 frames $\approx 1.86$ seconds. Zooming out, we observe peaks in correlation centred around 0.}\label{fig:cross-correlation}
\end{figure}

\section{Learning Rate and Scheduler Experiment Results}\label{app:learning_experiment_results}

\begin{figure}[H]
    \centering
    \includegraphics[width=1.0\textwidth]{figures/lr_search_cosine.png}
    \caption{Training graphs for the \emph{CRNN} with different learning rates and the \texttt{cosine} scheduler. The learning rate of $0.001$ seems to be the best, as it converges in a reasonable time and the validation accuracy increases in a stable fashion. While it may seem that running for more epochs may increase performance, this was not found to be the case empirically. The best model was often achieved around epoch 100.}\label{fig:lr_search_cosine}
\end{figure}

\begin{table}[H]
    \centering
    \begin{tabular}{lcccccc}
        \toprule
        lr & scheduler & acc & root & third & seventh & mirex \\
        \midrule
        0.01 & Cosine &  53.6 & 69.5 & 66.9 & 55.7 & 78.6 \\
        0.001 & Cosine & \emph{59.7} & \emph{78.3} & \emph{75.0} & \emph{62.0} & \emph{\textbf{79.8}} \\
        0.0001 & Cosine & 53.2 & 72.1 & 66.9 & 55.2 & 72.0 \\
        \midrule
        0.001 & Plateau & \textbf{59.9} & 78.4 & 75.2 & \textbf{62.2} & 79.7 \\
        0.001 & None & 59.8 & \textbf{78.7} &\textbf{75.5} & 62.0 & 78.8 \\
        \bottomrule
    \end{tabular}
    \caption{\emph{CRNN} model results with different learning rates and schedulers. Best results over learning rates are \emph{italicised} and best results over schedulers are in \textbf{boldface}. A learning rate of $0.001$ performs the best on all metrics. The differences between learning rate schedulers are so small that the choice between them is arbitrary. }\label{tab:crnn_lr}
\end{table}

\section{Revisiting the Spectrogram}\label{sec:spectrogram-results}

\subsection{Spectrogram Variants}\label{sec:spectrogram-variants}

It is standard practice in ACR to use a CQT as input. However, \citet{20YearsofACR} raise the question of whether the CQT is truly the best choice. They suggest that the pitch-folding of the CQT may distort the harmonic structure of notes. By contrast, \citet{MelodyTranscriptionViaGenerativePreTraining} use a mel-spectrogram in place of a CQT. 

I test four spectrogram variants in Table~\ref{tab:spectrograms}. These include the standard CQT, mel-spectrogram and linear spectrogram. I also calculate a chroma-CQT to test whether the model is using information from multiple octaves better than a hand-crafted algorithm. The chroma-CQT is calculated by summing CQT values across octaves. Spectrogram calculations are all implemented in \texttt{librosa}~\citep{librosa}. I use $216$ bins for the CQT and mel spectrograms and $2048$ fast Fourier transform (FFT) bins for the linear spectrogram with a hop length of $4096$ for all. 

Results show that CQTs are the best choice. This raises questions as to the validity of the conclusions drawn by \citet{MelodyTranscriptionViaGenerativePreTraining}. They claim that their generative features are better than hand-crafted features. However, they only compare to mel-spectrograms which may not perform as well as CQTs for the related task of melody recognition. The CQT is also better the chroma-CQT. We can be confident that the model is using information from multiple octaves more efficiently than the simply summing across octaves.

\begin{table}[h]
    \centering
    \begin{tabular}{lccccc}
        \toprule
        spectrogram & acc & root & third & seventh & mirex \\  
        \midrule
        CQT & \textbf{60.2} & \textbf{78.4} & \textbf{75.3} & \textbf{62.5} & \textbf{79.5} \\
        chroma-CQT & 50.1 & 71.4 & 65.7 & 52.0 & 69.8 \\
        mel & 52.7 & 69.1 & 66.3 & 54.6 & 70.6 \\
        linear & 51.2 & 66.1 & 63.0 & 53.1 & 73.8 \\
        \bottomrule
    \end{tabular}
    \caption{Results for CQT, chroma-CQT, mel and linear spectrograms. The CQT is certainly the best feature. The other all perform similarly on accuracy and \text{mirex}, but the chroma-CQT does comparatively at identifying thirds and sevenths. }\label{tab:spectrograms}
\end{table}

\subsection{Hop Lengths}\label{sec:hop-lengths}

Different hop lengths have been used to calculate the CQT ranging from 512~\cite{ACRLargeVocab1} up to 4096~\citep{StructuredTraining}. In previous experiments I have used a hop length of $4096$ as is used by the authors of \emph{CRNN}~\citep{StructuredTraining}. Shorter frames would reduce the number of transition frames but require more computational cost. If frame lengths are too short, the Fourier transform may not be able to capture the harmonic structure of the audio.

In Table~\ref{tab:hop_lengths}, I test the effect of different hop lengths on the model's performance. I use a CQT with $216$ bins and a hop length of $512$, $1024$, $2048$, $4096$, $8192$ and $16384$. Results indicate that performance is similar for hop lengths of $4096$ and below. Performance suffers for greater hop lengths. While it could be argued that $2048$ does better than $4096$, this difference is small enough that it is not worth the increased computational cost. Models trained with a hop size of $2048$ take at least twice as long to train and evaluate as those trained on a hop size of $4096$.

\begin{table}[h]
    \centering
    \begin{tabular}{lccccc}
        \toprule
        hop length & acc & root & third & seventh & mirex \\  
        \midrule
        512 & 60.1 & 78.3 & \textbf{75.5} & 62.4 & \textbf{80.0} \\
        1024 & 60.2 & \textbf{78.7} & 75.2 & 62.5 & 79.6 \\
        2048 & \textbf{60.3} & 78.5 & 75.2 & \textbf{62.6} & 79.6 \\
        4096 & 60.0 & 78.1 & 75.0 & 62.2 & 79.2 \\
        8192 & 57.9 & 76.2 & 72.9 & 60.1 & 79.3 \\
        16384 & 53.3 & 71.7 & 68.0 & 55.4 & 77.9 \\
        \bottomrule
    \end{tabular}
    \caption{Results over different hop lengths for CQT calculation. Any hop length in the range $512$ to $4096$ has similar performance. For frames that are any longer, performance suffers. This is likely caused by the requirement for the model to assign a single chord class to each frame. The longer the frame, the greater potential there is for multiple chords to be playing during the frame. }\label{tab:hop_lengths}
\end{table}


\section{Small vs Large Vocabulary}\label{app:small_vs_large_vocabulary}

Some initial experiments were conducted over a smaller vocabulary with $C=26$. This vocabulary includes a symbol for major and minor over each root and two special symbols, \texttt{N} and \texttt{X} for no chord and unknown chord respectively. This contrasts the much larger vocabulary with 14 chord qualities for each root which is used for the majority of the experiments. With this larger vocabulary, $C=170$.

Table~\ref{tab:small_vs_large_vocab} shows the results of the \emph{CRNN} trained over the smaller and larger vocabulary evaluated on the small vocabulary. The predictions of the model and the reference labels are all mapped to the small vocabulary before being evaluated in the same was as described in Section~\ref{sec:evaluation}. This test was to verify that the model trained on the larger vocabulary performs well competitively on the smaller vocabulary. If the model trained on the larger vocabulary performed poorly on the smaller vocabulary, it may be prudent to first try to improve performance on this smaller vocabulary. It may also be a sign that the larger vocabulary is too complex or that the more detailed annotations are inconsistent.

However, the table shows very similar performance between both models. This allows us to proceed with the larger vocabulary for the rest of the experiments. The larger vocabulary is also more consistent with the literature and allows for a model to produce far more interesting chord predictions than simply minor, major and root. 

\begin{table}[H]
    \centering
    \begin{tabular}{lccc}
        \toprule
        Vocab & $C$ & acc & root \\  
        \midrule
        small & $26$ & 76.7 & 80.1 \\
        large & $170$ & 76.0 & 79.1 \\
        \bottomrule
    \end{tabular}
    \caption{\emph{CRNN} with a small and large vocabulary. Metrics show similar performance between the two. Training on the large vocabulary does not prevent the model from learning how to classify the smaller vocabulary. Thus, I proceed with the larger vocabulary.}\label{tab:small_vs_large_vocab}
\end{table}

Note that the \texttt{mir\_eval} package also includes a \texttt{majmin} evaluation metric that compares chords over just the major and minor qualities. However, this is not quite the same as the test above due to subtleties in how \texttt{mir\_eval} chooses whether or not a chord is major or minor. It ends up ignoring many chords that could be mapped to these qualities in the smaller vocabulary. Coincidentally, the \emph{CRNN} with the default parameters attains a \texttt{majmin} accuracy of $76.0\%$ over the larger vocabulary. This further confirms that we need not continue to test on the smaller vocabulary. The  \texttt{majmin} metric is not used in the rest of the thesis as it is not as informative as the other metrics and the \texttt{third} metric is highly correlated with it.

\section{Chord Mapping}\label{app:chord_mapping}

Chords in Harte notation were mapped to the vocabulary with $C=170$ by first converting them to a tuple of integers using the Harte library. These integers represent pitch classes and are in the range 0 to 11 inclusive. They are transposed such that 0 is the root pitch. These pitch classes were then matched to the pitch classes of a quality in the vocabulary, similar to the work by \citet{StructuredTraining}. However, for some chords, this was not sufficient. For example, a \texttt{C:maj6(9)} chord would not fit perfectly with any of these templates due to the added 9th. Therefore, the chord was also passed through Music21's~\citep{music21} chord quality function which matches chords such as the one above to major. This function would not work alone as its list of qualities is not as rich as the one defined above. If the chord was still not matched, it was mapped to \texttt{X}. This additional step is not done by \citet{StructuredTraining} but gives more meaningful labels to roughly one third of the chords previously mapped to \texttt{X}.

\section{CRNN with CR2}\label{app:crnn_with_cr2}

\begin{table}[H]
    \centering
    \begin{tabular}{lccccccc}
        \toprule
        cr2 & acc & root & third & seventh & mirex & acc\textsubscript{class} & median\textsubscript{class} \\  
        \midrule
        on & 59.7 & \textbf{78.9} & \textbf{75.6} & 61.9 & \textbf{80.5} & 18.4 & 0.4 \\
        off & \textbf{60.2} & 78.4 & 75.3 & \textbf{62.5} & 79.5 & \textbf{19.4} & \textbf{1.1} \\
        \bottomrule
    \end{tabular}
    \caption{\emph{CRNN} with and without the added `CR2' decoder. Performance is very similar between the two. It could be argued that te model with CR2 on is better, but for simplicity, I proceed with the model without CR2. One could also argue that the effect of CR2 is similar to simply adding more layers to the GRU already present in the \emph{CRNN}.}
\end{table}

\section{A Long Run with SGD}\label{app:long_sgd}
\begin{figure}[H]
    \centering
    \includegraphics[width=1.0\textwidth]{figures/long_sgd_training_plot.png}
    \caption{Training graphs for the \emph{CRNN} trained with SGD, momentum 0.9, a learning rate of $0.001$ and the \texttt{cosine} scheduling for 2000 epochs. Convergence is reached but performance does not exceed that which is achieved by Adam over 150\,epochs. Furthermore, there is signifcant computational cost associated with running for 2000 epochs. I proceed with Adam for the remainder of experiments. }\label{fig:long_sgd}
\end{figure}


\section{Random Hyperparameter Search Sets}\label{app:random_hyperparameter_search_sets}

The random hyperparameter search for the \emph{CRNN} was done over the following variables and values:
\begin{itemize}
    \item \texttt{hidden\_size} $\in$ \{32, 64, 128, 256, 512\}
    \item \texttt{num\_layers} $\in$ \{1, 2, 3\}
    \item \texttt{segment\_length} $\in$ \{5, 10, 15, 20, 25, 30, 35, 40, 45\}
    \item \texttt{kernel\_size} $\in$ \{5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15\}
    \item \texttt{cnn\_layers} $\in$ \{1,2,\ldots,5\}
    \item \texttt{cnn\_channels} $\in$ \{1,2,\ldots,5\}
\end{itemize}

For each run, a value was selected for each hyperparameter, with each possible value equally likely.

% \section{CNN Hyperparameter Search}\label{app:cnn_hparams}

% \begin{table}[H]
%     \centering
%     \begin{tabular}{@{}cccccccccc@{}}
%         \toprule
%         $k$ & $l$ & $c$ & acc & root & third & seventh & mirex & acc\textsubscript{class} & median\textsubscript{class} \\  
%         \midrule
%         5 & 1 & 1 & 54.5 & 74.4 & 69.0 & 56.6 & 73.5 & 16.0 & 2.3 \\
%         5 & 3 & 5 & 57.0 & 76.9 & 72.5 & 59.2 & 77.6 & 18.9 & 3.0 \\
%         9 & 5 & 10 & \textbf{57.8} & \textbf{78.1} & \textbf{74.0} & \textbf{60.0} & \textbf{77.8} & \textbf{19.2} & \textbf{3.2} \\
%         \bottomrule
%     \end{tabular}
%     \caption{CNN hyperparameter search results. The best performing result for each metric is highlighted in \textbf{bold}. Performance increases with depth of the model. However, the performance increase is not significant between the two best performing models. Deeper CNNs were also trained, but were results did not improve and were too computationally intensive to run repeatedly.}
% \end{table}

\section{Confusion Matrix of CRNN over Roots}\label{app:cm_roots}

\begin{figure}[H]
    \centering
    \includegraphics[width=1.0\textwidth]{figures/confusion_matrix_roots.png}
    \caption{Performance is relatively stable across roots. The only outlier is the unknown chord symbol \texttt{X}. This is to bexpected given the ambiguous nature of the chord. }
    \label{fig:cm_roots}
\end{figure}

% \section{Accuracy vs Hop Length}\label{app:accuracy_vs_hop_length}

% \begin{figure}[H]
%     \centering
%     \includegraphics[width=0.5\textwidth]{figures/hop_length_vs_accuracy.png}
%     \caption{Accuracy vs hop length. Metrics are not directly comparable over hop lengths due to different likelihoods. However, the metrics are fairly consistent over different hop lengths, certainly over the region explored by the literature $[512,2048,4096]$. Every hop length tested is short enough to be more granular than chords, but not so short that the computed CQT is too noisy. We continue with the default hop length of $4096$, to be consistent with some of the literature while keeping computational cost low.}
%     \label{fig:accuracy_vs_hop_length}
% \end{figure}

\section{Incorrect Region Lengths With/Without Smoothing}\label{app:histogram_over_region_lengths}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.6\textwidth]{figures/incorrect_region_smoothing_histogram.png}
    \caption{Histogram over incorrect region lengths for the \emph{CRNN} with and without smoothing. An incorrect region is defined as a sequence if incorrect frames with correct adjacent of either end. Both distributions have a long-tail, with $26.7\%$ regions being of length 1 without smoothing. This raises concerns over the smoothness of outputs and requires some form of post-processing explored in Section~\ref{sec:decoding}. The distribution is more uniform with smoothing, with approximately half the very short incorrect regions.}
    \label{fig:histogram_over_region_lengths}
\end{figure}

\section{Accuracy over the Context}\label{app:accuracy_over_context}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{figures/accuracy_over_frames.png}
    \caption{Average frame-wise accuracy of the \emph{CRNN} model over the patch of audio. The model performs worse at the beginning and end of the patch of audio, as expected. However, the differences are only $~0.05$. We propose that the context on one side is enough for the model to attain the vast majority of the performance attained with bi-directional context. This plot supports our procedure of evaluating over the entire song at once. }\label{fig:crnn_context}
\end{figure}

\section{Accuracy vs Context Length of Evaluation}\label{app:accuracy_vs_context_length}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\textwidth]{figures/context_length_vs_accuracy.png}
    \caption{Accuracy with increasing segment length of validation set. The accuracy increases very slightly. I choose to continue evaluating over the entire song at once.}
    \label{fig:accuracy_vs_context_length}
\end{figure}

\section{HMM Smoothing Effect}\label{app:hmm_smoothing_effect}

\begin{figure}[H]
    \centering
    % \hspace{-1.5cm}
    \includegraphics[width=0.9\textwidth]{figures/hmm_smoothing_example.png}
    \caption{An example of the effect of the HMM on the predictions of the \emph{CRNN} model. The top plot shows the ground truth. The middle plot shows frame-wise predictions of the \emph{CRNN} without smoothing. The bottom plot shows the predictions after smoothing. Chords are coloured by their equivalent chord in the small vocabulary as it makes the plot easier to interpret. The original predictions contain many unnecessary and nonsensical chord transitions. These have been smoothed out by the HMM. The resulting chords appear more similar to the ground truth even if frame-wise accuracy has not changed much.}\label{fig:hmm_smoothing_example}
\end{figure}

\section{Weighted Loss Confusion MAtrix}\label{app:weighted_loss_confusion_matrix}

\begin{figure}[H]
    \centering
    \hspace{-1.5cm}
    \includegraphics[width=0.9\textwidth]{figures/confusion_matrix_difference.png}
    \caption{ Most of the diagonal entries in the confusion matrix increase. Recall on major7 qualities increases by $0.17$. The only qualities to decrease in recall are major, \texttt{N} and \texttt{X}. I conclude that weighting the loss does improve the model. The weighted model predicts \texttt{X} $2.2$ times less often. This may be how the weighted model improves class-wise metrics without sacrificing too much overall accuracy since \texttt{X} frames are ignored for evaluation.}\label{fig:hmm_smoothing_example}
\end{figure}

\section{Structured Loss Experiment Results}\label{app:structured_loss_experiment_results}
\begin{figure}[H]
    \centering
    \includegraphics[width=1.0\textwidth]{figures/structured_loss_accuracy.png}
    \caption{Effect of structured loss on the \emph{CRNN} model with varying $\gamma$. As we increase $\gamma$, accuracy improves but \texttt{mirex} behaves erratically, worsening at the higher end. The other metrics behave similarly to accuracy. I choose $\gamma=0.7$ based on peak accuracy. }\label{fig:structured_loss}
\end{figure}

\section{Details of Generative Feature Extraction}\label{app:generative_feature_extraction}

Overlapping $5$ second chunks of audio were fed through MusicGen in a batched fashion. This first requires passing the audio through the pre-trained Encodec audio tokeniser~\citep{Encodec}. These are then fed through the language model. I take the output logits as the representation for each frame. The model outputs logits in four `codebooks', each $2048$-dimensional vectors, intended to represent different granularities of detail in the audio. Audio segments are overlapped such that every frame has context from both directions. The multiple representations for each frame are averaged. Finally, these representations are upsampled. The model operates at a frame rate of 50Hz. To compute a representation with the same frame length as the CQT, I take the mean over the frames outputted by the model closest to the centre of the CQT frame. In case averaging over frames dampened the signal, I also tried linearly interpolating between the two closest frames outputted by the model. However, this was empirically found to perform slightly worse. Results are left to Appendix~\ref{app:linear_interpolation_vs_area_averaging}. This feature extraction required the use of NVIDIA RTX A6000 GPUs. The extraction process takes 4 hours for each model over the entire dataset.

\section{Upsampling Methods in Generative Feature Extraction}\label{app:linear_interpolation_vs_area_averaging}

\begin{table}[H]
    \centering
    \begin{tabular}{cccccc}
        \toprule
        upsample & accuracy & root & third & seventh & mirex  \\  
        \midrule
        area & \textbf{59.4} & \textbf{77.8} & \textbf{74.6} & \textbf{61.7} & \textbf{78.1} \\
        lerp & 58.4 & 77.7 & 74.0 & 60.6 & 78.2 \\
        \bottomrule
    \end{tabular}
    \caption{Comparison of generative feature extraction with linear interpolation and area averaging for the musicgen-large and a linear projection down to $64$ dimensions and averaging over the four codebooks. The results are very similar, but the area averaging method is slightly better in all metrics. I therefore choose to continue averaging over model frames in order to upsample to CQT frames.}\label{tab:linear_interpolation_vs_area_averaging}
\end{table}

\section{Generative Feature Low-Dimensional Projection}\label{app:projection_dimensionality}

\begin{table}[H]
    \centering
    \begin{tabular}{cccccc}
        \toprule
        $d$ & accuracy & root & third & seventh & mirex  \\  
        \midrule
        16   & 58.4       & 77.6       & \textbf{74.5} & 60.6       & 77.2       \\
        32   & 57.8       & 77.5       & 73.6          & 60.0       & 77.8       \\
        64   & \textbf{59.2} & 77.5    & 74.3          & \textbf{61.4} & \textbf{77.9} \\
        128  & 58.1       & 77.0       & 73.6          & 60.3       & 77.4       \\
        256  & 58.7       & 77.6       & 74.3          & 60.9       & 77.5       \\
        512  & 58.3       & \textbf{77.9} & 74.3      & 60.6       & 77.6       \\
        1024 & 58.4       & 76.7       & 73.5          & 60.7       & 76.7       \\ 
        \bottomrule
    \end{tabular}
    \caption{Generative feature extraction with different projection dimensions, $d$. All results use the musicgen-large model and average across the four codebooks. There are no large differences between the different dimension reductions. I take $d=64$ as it performs the best, but there is no evidence that this is not due to randomness in the optimisation process. }\label{tab:projection_dimensionality}
\end{table}

\section{Generative Features with Different Models}\label{app:generative_feature_extraction_models}

\begin{table}[H]
    \centering
    \begin{tabular}{lc}
        \toprule
        model  & acc  \\
        \midrule
        large  & \textbf{60.2} \\
        small  & 59.8 \\
        melody & 59.9 \\
        chord  & 59.7 \\
        \bottomrule
    \end{tabular}
    \caption{Results for generative features extracted from MusicGen~\citep{MusicGen} and MusiConGen~\citep{MusiConGen} models with different musicgen-models. Only accuracy is reported as the other metrics are qually similar. These models are musicgen-large, musicgen-small, musicgen-melody and MusiConGen, referred to as large, small, melody and chord respectively. There are very few differences between the models. This suggests that the small model has just as much information in its internal representations that is useful for identifying chords as the other models. Another point of note is the comparison between chord and melody. The chord model is a chord-conditioned fine-tuned version of melody. It is surprising that the chord-conditioning did not help compared to the non chord-conditioned model.}\label{tab:musicgen_pivot}
\end{table}

\section{Generative Features with Different Codebook Reductions}\label{app:generative_feature_extraction_reductions}

\begin{table}[H]
    \centering
    \begin{tabular}{lc}
        \toprule
        reduction   & accuracy \\  
        \midrule
        concat       & 58.9     \\
        codebook\_2  & 58.9     \\
        codebook\_3  & 57.5     \\
        codebook\_1  & 59.0     \\
        codebook\_0  & 58.1     \\
        avg          & \textbf{59.4}     \\
        \bottomrule
    \end{tabular}
    \caption{Accuracy for different codebook reduction methods. Other metrics are omitted as they do not provide more information. All results are for musicgen-large with reduction down to $64$ dimensions. Reductions of the form `codebook\_$n$' refer to training on codebook of index $n$ from the model. Performance is similar across reductions except for codebook\_0 and codebook\_3 which perform worse. I choose the averaging reduction based on maximum accuracy. }\label{tab:reduction_accuracy}
\end{table}

\section{Jazz Chord Progression Generation}\label{app:jazz_chord_progression_generation}

The theory of functional harmony is a set of rules that govern the relationships between chords in a piece of music. While these rules are not always followed, many chord progressions can be parsed and broken down into the rules that have been followed to create them. The rules are based on the relationships between the chords and the keys they are in. For example, a chord progression that moves from a tonic chord to a dominant chord is said to be following the rule of \emph{dominant function}. This is a common rule in jazz music and is often used to create tension and resolution in a piece of music.

I first decide whether we are in major or minor, each with probability $0.5$. I then uniformly sample a tonic from the set of notes in the Western chromatic scale. From this tonic, seven functional chords are decided before sequence generation. These are all probabilistic. For example, the tonic chord is always the tonic, but the dominant chord can be of \texttt{maj}, \texttt{7}, \texttt{sus4}, \texttt{aug} or \texttt{dim7} qualities. The probabilities are user-tuned but do not matter very much to the funcionality of the synthetic dataset. 

For chord sequence generation, various rules are followed in a probabilistic manner. Progressions have a random length, uniformly sampled in the range $[4, 10]$.

\begin{itemize}
    \item Tonic (I) may move to predominant chords (ii, IV, vi) or occasionally mediant (iii).
    \item Predominant chords (ii, IV) resolve to the dominant (V).
    \item Dominant (V) usually cadences back to tonic (I) or sometimes moves to vi.
    \item Tonic substitute (vi) leads to ii or iii.
    \item Mediants (iii) feed into vi.
    \item Unspecified or fallback transitions are routed toward ii to maintain forward motion.
\end{itemize}

A time-aligned chord sequence is then calculated in a similar format to that provided by the \texttt{jams} package. This assumes that each chord is played for one bar, that the BPM is always followed, and that MusiConGen simply loops over the chord progression if the end is reached. These assumptions were found to hold on manually inspected examples.

For further details and exact probabilities used, please refer to the provided code.\footnote{\url{https://github.com/PierreRL/LeadSheetTranscription/blob/main/src/data/synthetic_data/chord_sequence.py}}

\section{Calibration to Handle Distribution Shift}\label{app:calibration}

To correct for the distribution shift between synthetic training data and the \emph{pop} train split, I estimate the empirical class probabilities in each domain---\(P_{\text{train}}(y)\) and \(P_{\text{pop}}(y)\)---and rescale the modelâ€™s logits by the ratio
\begin{equation}
r(y)\;=\;\frac{P_{\text{pop}}(y)}{P_{\text{train}}(y)}.
\end{equation}

In order for calibration to be root invariant, I take the mean ratio over chords that share the same quality, and a single calibration factor \(r_q\) is applied to every chord with that quality.

Note that the model's outputs are in logits so the log ratio is added in implementation.

Figure~\ref{fig:calibration} shows the calibration of the model's outputs to account for distribution shift.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{figures/calibration_ratios.png}
    \caption{Calibration ratios $r_q$ of the model's outputs to account for distribution shift. The high ratio on rare chord qualities like \texttt{majmin} show that these qualities are much more common in the synthetic dta.}\label{fig:calibration}
\end{figure}

\section{Difference in Confusion Matrixces with Synthetic Data}\label{app:cm_synthetic_data}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{figures/confusion_matrix_synth.png}
    \caption{Element-wise difference in normalised confusion matrix of the model trained on synthetic data and \emph{pop} data versus just \emph{pop} data with a weighted loss function. There are some notable differences. Training on synthetic data strongly discourages the model from predicting \texttt{X} chords, which are not present in the synthetic data. Recall on \texttt{min7} qualities increases by $13\%$ and by $7\%$ on \texttt{dim} qualities. However, recall \texttt{hdim7} and \texttt{min6} worsens. In general, the model predicts \texttt{maj} less often for rarer qualities. Another interesting observation is that the synthetic data corrects many of the predictions on \texttt{7} qualities from erroneous predictions of \texttt{min} to erroneous predictions of \texttt{maj}. Something similar happens with \texttt{min7} qualities. It is hard to say which model is better. Indeed, their overall accuracies are the same.}\label{fig:cm_synthetic_data}
\end{figure}

\section{Maximum Lag Cross Correlation of Chord Transitions with Beats}\label{app:maximum_lag_cross_correlation_beats}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{figures/beats_max_lags.png}
    \caption{Maximum lag cross correlation of chord transitions with beats within a window of $0.3$ seconds. Almost all maximum lags occur between $-0.1$ and $0.1$ seconds.}\label{fig:maximum_lag_cross_correlation}
\end{figure}