\chapter{Experimental Setup}

This chapter outlines the datasets used in this work, the preprocessing applied to the audio and chord annotations, the evaluation metrics used to compare the models and details of the training process.

\section{Data}

Two ACT datasets were used in this work. The first dataset is simply referred to as the \emph{Pop} dataset, as much of the music in the dataset comes from the Billboard Hot 100 charts, or other explicitly chooses pop music from the last 70 years. The second dataset is the \emph{JAAH} (Jazz Annotations and Analysis of Harmony) dataset mentioned in Section~\ref{sec:background-acr}. While the chords annotations are publicly available on Github\footnote{https://github.com/smashub/choco}, the audio was kindly given to me by Andrea Poltronieri, a PhD student at the University of Bologna and the author of 'ChoCo' (the Chord Corpus)~\citep{Choco}. I spent the first weeks of the project contacting authors of several papers which have used these or other datasets and he was the only one helpful enough to provide me with the audio.

The rest of this chapter explains processing applied to the audio and chord annotations common to both datasets, before discussing details of the \emph{Pop} and \emph{JAAH} datasets relevant to each.

\subsection{Preprocessing}

\subsubsection{Audio to CQT}\label{sec:audio-to-cqt}

The audio was first converted to a Constant-Q Transform (CQT) representation explained in Section~\ref{sec:background-features}. The CQT was computed using the \texttt{librosa} library~\citep{librosa}, using the \texttt{.cqt} function. A sampling rate of 44100Hz was used, with a hop size of 4096, and 36 bins per octave, 6 octaves and a fundamental frequency corresponding to the note \texttt{C1}. These parameters were chosen to be consistent with previous works~\citep{StructuredTraining}. This returns a complex-valued matrix containing phase, frequency and amplitude information. Phase information was discarded by taking the absolute value before being converted from amplitude to decibels (dB), equivalent to taking the logarithm. An alternative choice of frame length could be to use some fraction of a bar. For example, \citet{MelodyTranscriptionViaGenerativePreTraining} use 1/16th of a bar. However, most work in the literature opts for a constant hop size. We followed this convention as a hop size which varies over songs introduces a new source of error in identifying the tempo and meter of a song, which the authors note as sometimes being problematic to their method.

This leads to a CQT matrix of size $216 \times F$ where 216 is the number of frequency bins and $F$ is the number of frames in the song. The number of frames can be calculated as $F = \lceil \frac{44100}{4096} L  \rceil$ where $L$ is the length of the song in seconds, 44100 is the sampling rate in Hertz (Hz) and 4096 is hop size in samples. A 3 minute song has just under 2000 frames. To save on computational time, the CQT was pre-computed into a cached dataset rather than re-computing each CQT on every run.

\subsubsection{Chord Annotations}\label{sec:chord-annotations}

The chord annotation of a song is represented as a sorted dictionary, where each entry contains the chord label, the start time and duration. The chord label is represented as a string in Harte notation~\citep{HarteNotation}. For example, C major 7 is \texttt{C:maj7} and A half diminished 7th in its second inversion is \texttt{A:hdim7/5}. However, the majority chords in the dataset are not this complicated. The notation also includes \texttt{N} which signifies that no chord is playing. 

This annotation is too flexible to be used as directly as a target for a machine learning model trained on limited data. This would lead to thousands of classes, many of which would appear only once. Instead, we define two chord vocabularies. The first is a simple chord vocabulary which contains only the major and minor quality for each root and a no chord symbol \texttt{N}. Chords outside the vocabulary are mapped to \texttt{X}, a dedicated unknown symbol. For example, \texttt{C:maj7} is mapped to \texttt{C:maj} while \texttt{A:hdim7/5} is mapped to \texttt{X}. Letting $C$ be the size of the chord vocabulary, $C=26$ for the small vocabulary. The second is a more complex vocabulary, which contains 14 qualities for each root: major, minor, diminished, augmented, minor 6, major 6, minor 7, minor-major 7, major 7, dominant 7, diminished 7, half diminished 7, suspended 2, suspended 4. Additionally, we retain the \texttt{N} and \texttt{X} symbols totalling $C=12\cdot14 + 2 = 170$ chord labels. This vocabulary or similar vocabularies have been used by much of the literature~\citep{StructuredTraining,FourTimelyInsights,ACRLargeVocab1}.

Chords in Harte notation were mapped to this vocabulary by first converting them to a tuple of integers using the Harte library. These integers represent pitch classes and are in the range 0 to 11 inclusive. They are transposed such that 0 is the root pitch. These pitch classes were then matched to the pitch classes of a quality in the vocabulary, similar to the work by \citet{StructuredTraining}. However, for some chords this was not sufficient. For example, a \texttt{C:maj6(9)} chord would not fit perfectly with any of these templates due to the added 9th. Therefore, the chord was also passed through Music21's~\citep{music21} chord quality function which matches chords such as the one above to major. This function would not work alone as its list of qualities is not as rich as the one defined above. If the chord was still not matched, it was mapped to \texttt{X}. This additional step is not done by \citet{StructuredTraining} but gives more meaningful labels to roughly one third of the chords previously mapped to \texttt{X}.

\subsection{Pop Dataset}

The \emph{Pop} dataset consists of songs from the \emph{Mcgill Billboard}, \emph{Isophonics}, \emph{RWC-Pop} and \emph{USPop} datasets mentioned in Section~\ref{sec:background-acr}. This collection was originally proposed in work by \citet{FourTimelyInsights} in order to bring together most of the known datasets for chord recognition. The dataset consists of a subset of the above datasets filtered for duplicates and selected for those with annotations available. In total, there are 1,217 songs. The dataset was provided with obfuscated filenames and audio as \texttt{.mp3} files and annotations as \texttt{.jams} files~\citep{JAMS}. 

\subsubsection{Data Integrity}\label{sec:data-integrity}

Several possible sources of error in the dataset are investigated below.

\textbf{Duplicates:} Files were renamed using provided metadata identifying them by artist and song title. This is done to identify duplicates in the dataset. The dataset was filtered for duplicates, of which there was one: Blondie's `One Way or Another' which had two different recordings. The duplicate was removed from the dataset to as validation and testing should ensure the model generalises across songs, not across different recordings of songs. Further duplicates may exist under different names but throughout the project no other duplicates were found. Automatic analysis of the audio could help, although cannot be guaranteed to find different versions of the same song. As such, we proceed with the assumption that there are no duplicates in the dataset.

\textbf{Chord-Audio Alignment:} 10 songs were manually investigated for alignment issues. This was done by listening to the audio and comparing it to the annotations directly at various points in the song, with a focus on the beginning. It became apparent that precise timings of chord changes are ambiguous. The annotations aired on the side of being slightly early but were all certainly good annotations, with detailed chord labellings including inversions and upper extensions.

Automatic analysis of the alignment of the audio and chord annotations was also done using cross-correlation of the derivative of the CQT features of the audio over time and the chord annotations, varying a time lag. A maximum correlation at a lag of zero would indicate good alignment as the audio changes at the same time as the annotation. The derivative of the CQT in the time dimension was estimated using \texttt{librosa}'s \texttt{librosa.feature.delta} function. The chord annotations were converted to a binary vector, where each element corresponds to a frame in the CQT, and is 1 if a chord change occurs at that frame, and 0 otherwise. Both the CQT derivatives and binary vectors were normalised by subtracting the mean and dividing by the standard deviation. Finally, cross-correlation was computed using \texttt{numpy}'s \texttt{numpy.correlate} function. A typical cross-correlation for a song is shown in Figure~\ref{fig:cross-correlation}. We can see that the cross-correlation repeats every 20 frames or so. Listening to the song, we can interpret this as one bar-length as drum transients will be highly correlated every bar. 

\begin{figure}[H]
    \centering
    \includegraphics[width=1.0\textwidth]{figures/cross_correlation.png}
    \caption{Cross-correlation of the derivative of the CQT of the audio and the chord annotations for a single song. We can see correlation peaking in regular intervals of around 20 frames, which corresponds to one bar length in this song. Zooming out, we observe peaks in correlation centred around 0.}\label{fig:cross-correlation}
\end{figure}

To check alignment across the dataset, we can plot a histogram over songs of the lag of the maximum cross-correlations. If we further assume that the annotations are not incorrect by more than 10 seconds, we can restrict our maximum correlation search to a window of 100 frames either side of 0. A histogram of maximum-lags per song is shown in Figure~\ref{fig:durations-and-lags} where the maximum is within a window of 100. This reduction does not change the shape of the picture. Rather, focusing on a reduced set of lags allows more detail to be visible. The majority of songs have a maximum lag in the region of 0, with a few outliers. This can be attributed to noise. A final check was done by looking at the difference in length of the audio files and chord annotations. A histogram of differences in length is also shown in the figure. Furthermore, the majority of songs have a difference in length of 0, with a few outliers, almost all less than a second. This evidence combined with the qualitative analysis was convincing enough to leave the annotations as they are for training.

\begin{figure}[H]
    \centering
    \includegraphics[width=1.0\textwidth]{figures/duration_diffs_and_lags.png}
    \caption{Cross-correlation of the derivative of the CQT of the audio and the chord annotations for a single song. The x-axis is the lag in frames and the y-axis is the correlation. The plot repeats every 100 frames, which corresponds to 4 bars.}\label{fig:durations-and-lags}
\end{figure}

\textbf{Incorrect and Subjective Annotations:} Throughout manual listening, no obviously wrong annotations were found. However, looking at songs which the first trained models perform the worst on using the \texttt{mirex} metric, three songs stick out. `Lovely Rita' by the Beatles, `Let Me Get to Know You' by Paul Anka and `Nowhere to Run' by Martha Reeves and the Vandellas all had scores below $0.05$. In these songs, the model consistently guessed chords one semitone off, as if it thought the song was in a different key. Upon listening, it became clear that the tuning was not in standard A440Hz for the first two songs and the key of the annotation was wrong for the other. These songs were kept in to maintain consistency with previous works and are unlikely to skew reported metrics much as explained in Section~\ref{sec:evaluation}. No other songs were found to have such issues.

Chord annotations are inherently subjective to some extent. Detailed examples in \emph{Pop} are given by \citet{FourTimelyInsights}. They also note that there are several songs in the dataset of questionable relevance to ACR, as the music itself is not well-explained by chord annotations. However, these are kept in for consistency with other works as this dataset is often used in the literature. Some works decide to use the median as opposed to the mean accuracy in their evaluations in order to counteract the effect of such songs on performance~\citep{StructuredTraining}. We think that this is unnecessary as the effect of these songs is likely to be small and we do not wish to inflate our results. Further evidence is given in Section~\ref{sec:evaluation}.

\subsubsection{Chord Distribution}

Much of the recent literature has focused on the long tail of the chord distribution, using a variety of methods to attempt to address the issue~\citep{BalanceRandomForestACR,CurriculumLearning}. It is first helpful to understand the distribution of chords in the datasets, shown in Figure~\ref{fig:chord-distribution}. The distribution is broken down both by root and by quality, only for the larger chord vocabulary. The smaller vocabulary does not have the same long-tailed distribution. The plots show that the distribution over qualities is highly skewed, with major and minor chords making up the majority of the dataset and qualities like majorminor and diminished 7th chords having two to three orders of magnitude fewer frames. Another display over chord qualities can be found in the work by \citet{ACRLargeVocab1}. The distribution over roots is far less skewed, although there is a preference for chords for keys like C, D or E and fewer in C\# or F\#.

\begin{figure}[H]
    \centering
    \includegraphics[width=1.0\textwidth]{figures/chord_distribution.png}
    \caption{Chord distributions in the \emph{Pop} dataset. The plots show both the raw counts in terms of frames and the duration in seconds for each chord root/quality. Note that the y-axis on qualities is in log-scale. We observe that the qualities are very imbalanced, with \texttt{maj} the most popular, but that roots are relatively balanced.}\label{fig:chord-distribution}
\end{figure}


\subsection{JAAH Dataset}

TODO: I was warned by Andrea that the JAAH dataset has not been as commonly used as dataset the Billboard dataset. Therefore he could not guarantee that the audio was aligned for this dataset.

- As yet, JAAH is unused in this work
- Data was received as \texttt{.flac} files which were first converted to \texttt{.mp3} files to be in line with the Billboard dataset
- Comparison of the two datasets
- Description of the JAAH dataset and its use in this work
- Intended to be used as a test set to test the synthetic data generation.

\section{Evaluation}\label{sec:evaluation}

We evaluated models using the \texttt{mir\_eval} library~\citep{mir_eval}. This library provides a variety of metrics for evaluating the performance of chord recognition models. The metrics used in this work follow standard practice in using \texttt{root}, \texttt{third}, \texttt{seventh} and \texttt{mirex} accuracies. For a given hypothesis and reference chord, these metrics all return 1 if the root/third/seventh/three notes of the chord match and 0 otherwise. They return -1 if the true label is \texttt{X}. \texttt{X} is a highly ambiguous class with a wide variety roots and qualities that are mapped to it. From a musician's perspective, it would be better to have a reasonable guess at the chord rather than \texttt{X} which provides little information. Therefore, the model should not be penalised on frames with true label \texttt{X} and as such, these were ignored for all evaluation.

We also looked at the overall frame-wise accuracy, referred to as \texttt{frame} or simply accuracy. We chose not to use \texttt{majmin}, \texttt{triad} or \texttt{tetrad} as these are highly correlated with the previous metrics. This makes sense from a theoretical standpoint as the third and seventh are strong indicators of the triad and tetrad of the chord. This was also verified empirically on some initial results, where the \texttt{third} and \texttt{majmin} accuracies were always within $\pm 0.01$ of one another.

For the above metrics, the mean is computed for each song, and both the mean and median of the mean across songs were returned. Values are in the range $[0,1]$. The mean and median were found to within $\pm 0.01$ on all values observed. The median was consistently slightly higher likely due to there being more negative outliers than positive outliers. Some of these outliers are those previously identified as having incorrect annotations in Section~\ref{sec:data-integrity} or could be those identified as being unsuitable for chordal analysis~\citep{FourTimelyInsights}. We report only the mean throughout this work. This was chosen in part for being more commonly used in recent literature and in part to measure how well the model performs well enough on all songs, which the mean is a better estimate of. In any case, the similarity between median and mean results in small differences between the two. These metrics can be formally defined as follows:

\[ M(f) = \frac{1}{N} \sum_{i=1}^{N} \frac{1}{|\{t\in T : y_{i,t} \neq \texttt{X}\}|} \sum_{t\in T : y_{i,t} \neq \texttt{X}} f(y_{i,t}, \hat{y}_{i,t})\]

where $M$ is the metric in question, $f$ is a function which calculates the metric for a pair of reference and hypothesis chord labels, defined for each metric as above. $N$ is the number of songs, $T$ is the set of frames in a song, $y_{i,t}$ is the true chord at frame $t$ of song $i$, and $\hat{y}_{i,t}$ is the predicted chord at frame $t$ of song $i$.

For some experiments, we look at two more metrics. These are the mean and median class-wise accuracies, called \texttt{class}\textsubscript{mean} and \texttt{class}\textsubscript{median} respectively. These are the mean/median accuracy over each chord class. These metrics are formally defined as follows:

\[
\text{class}\textsubscript{mean} = \frac{1}{C} \sum_{c=1}^{C} \text{acc}(c), \quad
\text{class}\textsubscript{median} = \text{median}_{c=1}^{C} [\text{acc}(c)]
\]

where $\text{acc}(c)$ is the accuracy of class $c$ defined as:

\[\text{acc}(c) = \frac{\sum_{i=1}^{N} \sum_{t\in T : y_{i,t} \neq \texttt{X}} \mathbbm{1}_{(y_{i,t} = c \land \hat{y}_{i,t} = c)}}{\sum_{i=1}^{N} \sum_{t\in T : y_{i,t} \neq \texttt{X}} \mathbbm{1}_{(y_{i,t} = c)}}\]

where $C$ is the number of chord classes, and $N$, $T$, $y_{i,t}$ and $\hat{y}_{i,t}$ are as defined before.

These metrics were intended to measure the model's performance on the long tail of the chord distribution. It is important to measure both the mean and median as the distribution of accuracies over classes is highly skewed. These last two metrics are the only ones that take into account the entire evaluation dataset at once rather than on a per-song basis. Some songs may contain few or no chords of a particular class or quality, making the per-song metrics very noisy for class-wise metrics. We do not also compute \emph{quality}-wise accuracies as seen in other work~\citep{CurriculumLearning}. Compared to \texttt{class}\textsubscript{mean}, this metric only adds a weighted average over roots. As roots as fairly balanced, this would not add much information.

For the majority of experiments, the metrics on the validation set are used to compare performance. The test set is held out for use only to compare the final performances of selected models.

Finally, other evaluation tools were used, such as confusion matrices and accuracies over particular chord qualities and classes. Qualitative analysis was carried out to better understand the model's behaviour and to identify any systematic errors. We conclude our evaluation by looking at the predictions of the model in the context of its use for musicians or musical researchers. 

\section{Training}\label{sec:training}

Three variants of the dataset were used for training, validation and testing. For training, an epoch consisted of randomly sampling a patch of audio from each song in the training set. The length of this sample was kept as a hyperparameter, set to $28$ seconds for the majority of experiments. For testing, the entire song was used as performance was found to be marginally better if the model was allowed to see the entire song at once. This is later discussed in Section~\ref{sec:crnn_performance_across_context}. Because songs are of variable length, songs were split into patches of the same length as the training patches when validating mid-way through training. This meant that computation could be parallelised without as many padded frames as validating on the entire song at once. For all variants, frames in the batch were padded to the maximum length of the batch and padded frames were ignored for loss or metric calculation.

The models were trained using the \texttt{PyTorch} library~\citep{pytorch}. All final experiments were run on the ML Teaching Cluster with a variety of NVIDIA GPUs, mostly GTX 1080's (10GB VRAM) and RTX A6000's (48GB VRAM) depending on the size of experiments. Unless stated otherwise, models were trained with the Adam optimiser~\citep{adam} with a learning rate of $0.001$ and pytorch's \texttt{CosineAnnealingLR} scheduler, set to reduce the learning rate to 1/10th of its initial value over the run. Models were trained to minimise the cross entropy loss between the predicted chord and the true chord distributions. We used a batch size of 64 for a maximum of 150 epochs unless stated otherwise. This batch size was based on experiments testing the speed of computation of an epoch which are omitted for lack of relevance. Validation through training was only conducted every 5 epochs in order to save on computation time. Optionally, training was stopped early if the validation loss did not improve for 25 epochs. The model was saved whenever the validation loss improved. Each training run took approximately 30 minutes of GPU time. 

For the majority of experiments, a random 60/20/20\% training/validation/test split was used. This contrasts much of the literature which uses a 5-fold cross validation split introduced by \citet{FourTimelyInsights}. We did not maintain this split in order to obtain clean estimators of the generalisation error using the held-out test set. The split was kept constant across experiments. This unfortunately makes our results less comparable to those reported by the literature. However, we only  Later, models were re-trained on the combined training and validation sets and tested on the test set. In Chapter~\ref{chap:synthetic_data}, the models were trained on the entire \emph{Pop} dataset and tested on the \emph{JAAH} dataset.