{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import autorootcwd\n",
    "from tqdm import tqdm\n",
    "from enum import Enum\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import mir_eval\n",
    "\n",
    "from src.models.base import BaseACRModel\n",
    "from src.data.dataset import ChordDataset\n",
    "from src.utils import id_to_chord\n",
    "\n",
    "class EvalMetric(Enum):\n",
    "    \"\"\"\n",
    "    Defines an ENUM of evaluation metrics used in this project from the mir_eval library.\n",
    "\n",
    "    Attributes:\n",
    "        ROOT: Chord root evaluation metrics.\n",
    "        MAJMIN: Major/minor evaluation metrics.\n",
    "        MIREX: MIREX evaluation metric - returns 1 if at least three pitches are in common.\n",
    "        THIRD: Third evaluation metric - returns 1 if the third is in common.\n",
    "        SEVENTH: Seventh evaluation metric - returns 1 if the seventh is in common.\n",
    "    \"\"\"\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.name\n",
    "    \n",
    "    ROOT = \"root\"\n",
    "    MAJMIN = \"majmin\"\n",
    "    MIREX = \"mirex\"\n",
    "    THIRD = \"third\"\n",
    "    SEVENTH = \"seventh\"\n",
    "\n",
    "    def eval_func(self):\n",
    "        if self == EvalMetric.ROOT:\n",
    "            return mir_eval.chord.root\n",
    "        elif self == EvalMetric.MAJMIN:\n",
    "            return mir_eval.chord.majmin\n",
    "        elif self == EvalMetric.MIREX:\n",
    "            return mir_eval.chord.mirex\n",
    "        elif self == EvalMetric.THIRD:\n",
    "            return mir_eval.chord.thirds\n",
    "        elif self == EvalMetric.SEVENTH:\n",
    "            return mir_eval.chord.sevenths\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid evaluation metric: {self}\")\n",
    "\n",
    "    def evaluate(self, hypotheses: torch.Tensor, references: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Evaluate a model on a dataset split using a list of evaluation metrics.\n",
    "\n",
    "        Args:\n",
    "            hypotheses (torch.Tensor): The model's chord predictions as ids. Shape (B, frames)\n",
    "            references (torch.Tensor): The ground truth chord labels as ids. Shape (B, frames)\n",
    "\n",
    "        Returns:\n",
    "            metrics (torch.Tensor): A tensor of evaluation metrics and their values. Shape (B, frames)\n",
    "        \"\"\"\n",
    "        \n",
    "        # Initialize the evaluation metrics tensor. Shape (num_batches, num_frames)\n",
    "        metrics = torch.zeros_like(hypotheses, dtype=torch.float32)\n",
    "\n",
    "        # Iterate over the batch of chord predictions and ground truth labels\n",
    "        ref_labels = []\n",
    "        hyp_labels = []\n",
    "        for i in range(hypotheses.shape[0]):\n",
    "            # Convert the chord labels from indices to strings\n",
    "            ref_labels.extend([id_to_chord(id) for id in references[i]])\n",
    "            hyp_labels.extend([id_to_chord(id) for id in hypotheses[i]])\n",
    "\n",
    "        # Evaluate the chord labels using the evaluation metric\n",
    "        metrics = torch.from_numpy(self.eval_func()(ref_labels, hyp_labels))\n",
    "        \n",
    "        # Reshape the metrics tensor to include the batch dimension again\n",
    "        metrics = metrics.reshape(hypotheses.shape)\n",
    "        \n",
    "        return metrics\n",
    "\n",
    "def evaluate_model(\n",
    "        model: BaseACRModel,\n",
    "        dataset: ChordDataset,\n",
    "        evals: list[EvalMetric] = [EvalMetric.ROOT, EvalMetric.MAJMIN, EvalMetric.MIREX, EvalMetric.THIRD, EvalMetric.SEVENTH]\n",
    "    ) -> dict[str, float]:\n",
    "    \"\"\"\n",
    "    Evaluate a model on a dataset split using a list of evaluation metrics.\n",
    "\n",
    "    Args:\n",
    "        model (BaseACRModel): The model to evaluate.\n",
    "        dataset (ChordDataset): The dataset to evaluate on.\n",
    "        evals (list[EvalMetrics]): The evaluation metrics to use. Defaults to [EvalMetrics.ROOT, EvalMetrics.MAJMIN, EvalMetrics.MIREX, EvalMetrics.CHORD_OVERLAP, EvalMetrics.CHORD_LABEL].\n",
    "\n",
    "    Returns:\n",
    "        metrics (dict[str, float]): A dictionary of evaluation metrics and their values.\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize the evaluation metrics dictionary\n",
    "    metrics = {}\n",
    "\n",
    "    data_loader = DataLoader(dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "    for eval in evals:\n",
    "        # Initialize the evaluation metric\n",
    "        metrics[eval.value] = 0.0\n",
    "\n",
    "    # Evaluate the model on the data loader\n",
    "    for batch_features, batch_labels in tqdm(data_loader):\n",
    "\n",
    "        # Get the chord predictions from the model\n",
    "        predictions = model.predict(batch_features)\n",
    "\n",
    "        # Evaluate the model on the sample using the evaluation metrics\n",
    "        for eval in evals:\n",
    "            metrics[eval.value] += torch.mean(eval.evaluate(predictions, batch_labels))\n",
    "\n",
    "    # Calculate the average evaluation metrics\n",
    "    for eval in evals:\n",
    "        metrics[eval.value] /= len(dataset)\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data.dataset import ChordDataset\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "dataset = ChordDataset()\n",
    "\n",
    "# Split the dataset into train and test\n",
    "train_size = int(0.999 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "\n",
    "torch.manual_seed(42)\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1712])\n",
      "torch.Size([1, 1712])\n",
      "torch.Size([1, 1712])\n",
      "torch.Size([1, 1712])\n",
      "torch.Size([1, 1712])\n",
      "torch.Size([1, 1712])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 1/2 [00:00<00:00,  2.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1712])\n",
      "torch.Size([1, 1712])\n",
      "torch.Size([1, 1712])\n",
      "torch.Size([1, 1712])\n",
      "torch.Size([1, 3880])\n",
      "torch.Size([1, 3880])\n",
      "torch.Size([1, 3880])\n",
      "torch.Size([1, 3880])\n",
      "torch.Size([1, 3880])\n",
      "torch.Size([1, 3880])\n",
      "torch.Size([1, 3880])\n",
      "torch.Size([1, 3880])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:01<00:00,  1.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3880])\n",
      "torch.Size([1, 3880])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'root': tensor(0.0763, dtype=torch.float64),\n",
       " 'majmin': tensor(0.0395, dtype=torch.float64),\n",
       " 'mirex': tensor(0.0395, dtype=torch.float64),\n",
       " 'third': tensor(0.0395, dtype=torch.float64),\n",
       " 'seventh': tensor(0.0395, dtype=torch.float64)}"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.models.random_baseline import RandomACR\n",
    "\n",
    "# Evaluate the random model on the test dataset\n",
    "model = RandomACR()\n",
    "\n",
    "metrics = evaluate_model(model, test_dataset)\n",
    "metrics"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "UG4Diss",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
