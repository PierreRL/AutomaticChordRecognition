{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [],
            "source": [
                "%reload_ext autoreload\n",
                "%autoreload 2"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [],
            "source": [
                "import autorootcwd\n",
                "import torch\n",
                "import os\n",
                "import numpy as np\n",
                "from tqdm import tqdm\n",
                "from typing import Dict, List\n",
                "\n",
                "import matplotlib.pyplot as plt\n",
                "from torch.utils.data.dataloader import DataLoader\n",
                "from sklearn.metrics import  accuracy_score\n",
                "\n",
                "from src.utils import get_torch_device, collate_fn, NUM_CHORDS, get_split_filenames\n",
                "from src.models.crnn import CRNN\n",
                "from src.models.base_model import BaseACR\n",
                "from src.data.dataset import FullChordDataset\n",
                "from src.eval import EvalMetric"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [],
            "source": [
                "from functools import lru_cache\n",
                "from src.utils import chord_to_id, id_to_chord\n",
                "\n",
                "@lru_cache(maxsize=None)\n",
                "def large_to_small_vocab_id(id: int) -> int:\n",
                "    \"\"\"\n",
                "    Converts a large vocabulary chord id to a small vocabulary chord id.\n",
                "\n",
                "    Args:\n",
                "        id (int): The large vocabulary chord id.\n",
                "\n",
                "    Returns:\n",
                "        int: The small vocabulary chord id.\n",
                "    \"\"\"\n",
                "\n",
                "    chord = id_to_chord(id, use_small_vocab=False)\n",
                "    return chord_to_id(chord, use_small_vocab=True)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Load Models"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [],
            "source": [
                "from src.data.dataset import IndexedDataset\n",
                "from src.utils import collate_fn_indexed, chord_to_id, get_chord_seq, get_synthetic_chord_seq \n",
                "import mir_eval\n",
                "from src.eval import bootstrap_mean_ci, compute_aggregated_class_metric\n",
                "\n",
                "def evaluate_model_large_vs_small(\n",
                "    model: BaseACR,\n",
                "    dataset: FullChordDataset,\n",
                "    evals: List[EvalMetric] = [\n",
                "        EvalMetric.ROOT,\n",
                "        EvalMetric.MIREX,\n",
                "        EvalMetric.THIRD,\n",
                "        EvalMetric.SEVENTH,\n",
                "        EvalMetric.MAJMIN,\n",
                "        EvalMetric.ACC,\n",
                "    ],\n",
                "    is_small: bool = False,\n",
                "    batch_size: int = 32,\n",
                "    device: torch.device = None,\n",
                "    log_calibration: torch.Tensor = None,\n",
                ") -> dict:\n",
                "    \"\"\"\n",
                "    Evaluate a model using continuous, song-based metrics computed with mir_eval.\n",
                "    \"\"\"\n",
                "    torch.set_grad_enabled(False)\n",
                "    if device is None:\n",
                "        device = get_torch_device()\n",
                "    model.to(device)\n",
                "    model.eval()\n",
                "\n",
                "    filenamed_dataset = IndexedDataset(dataset)\n",
                "\n",
                "    data_loader = DataLoader(\n",
                "        filenamed_dataset,\n",
                "        batch_size=batch_size,\n",
                "        shuffle=False,\n",
                "        collate_fn=collate_fn_indexed,\n",
                "    )\n",
                "\n",
                "    print(\"Evaluating model...\")\n",
                "    song_predictions = []\n",
                "\n",
                "    for (batch_cqts, batch_gens, batch_labels), indices in tqdm(\n",
                "        data_loader, desc=\"Predicting\"\n",
                "    ):\n",
                "        batch_cqts = batch_cqts.to(device)\n",
                "        if batch_gens is not None and batch_gens.nelement() > 0:\n",
                "            batch_gens = batch_gens.to(device)\n",
                "        batch_labels = batch_labels.to(device)\n",
                "\n",
                "        valid_mask = torch.logical_and(\n",
                "                batch_labels != -1, batch_labels != chord_to_id(\"X\", use_small_vocab=True)\n",
                "        )  # Mask out -1 and X labels\n",
                "\n",
                "        ignore_mask = batch_labels != -1\n",
                "\n",
                "        if hasattr(model, \"use_generative_features\") and model.use_generative_features:\n",
                "            predictions = model.predict(\n",
                "                batch_cqts, batch_gens, mask=valid_mask, device=device, log_calibration=log_calibration\n",
                "            )\n",
                "        else:\n",
                "            crf_mask = valid_mask.clone()\n",
                "            crf_mask[:, 0] = True  # Ensure the first frame is always valid\n",
                "            predictions = model.predict(batch_cqts, mask=crf_mask, device=device, log_calibration=log_calibration)\n",
                "\n",
                "        predictions = predictions.cpu().numpy()\n",
                "\n",
                "        if not is_small:\n",
                "            # Convert large vocabulary predictions to small vocabulary\n",
                "            predictions = np.vectorize(large_to_small_vocab_id)(predictions)\n",
                "\n",
                "        for i in range(predictions.shape[0]):\n",
                "            song_predictions.append(\n",
                "                {\n",
                "                    \"pred_ids\": predictions[i][ignore_mask[i].cpu().numpy()].tolist(),\n",
                "                    \"idx\": indices[i],\n",
                "                }\n",
                "            )\n",
                "\n",
                "    song_metric_scores = {m: [] for m in evals}\n",
                "    song_transition_counts = []\n",
                "    song_agg_data = []\n",
                "\n",
                "    for song in tqdm(song_predictions, desc=\"Evaluating\"):\n",
                "        filename = dataset.get_filename(song[\"idx\"])\n",
                "        is_synthetic = dataset.is_synthetic(song[\"idx\"])\n",
                "        pred_labels = [id_to_chord(x, use_small_vocab=True) for x in song[\"pred_ids\"]]\n",
                "\n",
                "        # Get estimated beat boundaries (from the features) and reference beat boundaries.\n",
                "        est_beats = dataset.get_beats(song[\"idx\"])\n",
                "\n",
                "        # Get ground-truth chord sequence (one label per reference beat interval).\n",
                "        if is_synthetic:\n",
                "            ref_labels, ref_beats = get_synthetic_chord_seq(\n",
                "                filename, override_dir=f\"{dataset.synthetic_input_dir}/chords\", use_small_vocab=True\n",
                "            )\n",
                "        else:\n",
                "            ref_labels, ref_beats = get_chord_seq(\n",
                "                filename, override_dir=f\"{dataset.input_dir}/chords\", use_small_vocab=True\n",
                "            )\n",
                "\n",
                "        # Convert beat boundaries into intervals.\n",
                "        est_intervals = np.column_stack((est_beats[:-1], est_beats[1:]))\n",
                "        ref_intervals = np.column_stack((ref_beats[:-1], ref_beats[1:]))\n",
                "\n",
                "        # Adjust the estimated intervals so that they span the same range as the reference intervals.\n",
                "        adjusted_est_intervals, est_labels = mir_eval.util.adjust_intervals(\n",
                "            est_intervals,\n",
                "            pred_labels,\n",
                "            ref_intervals.min(),\n",
                "            ref_intervals.max(),\n",
                "            mir_eval.chord.NO_CHORD,\n",
                "            mir_eval.chord.NO_CHORD,\n",
                "        )\n",
                "\n",
                "        merged_intervals, merged_ref, merged_est = (\n",
                "            mir_eval.util.merge_labeled_intervals(\n",
                "                ref_intervals, ref_labels, adjusted_est_intervals, est_labels\n",
                "            )\n",
                "        )\n",
                "        durations = mir_eval.util.intervals_to_durations(merged_intervals)\n",
                "\n",
                "        merged_ref = np.array(merged_ref)\n",
                "        merged_est = np.array(merged_est)\n",
                "        durations = np.array(durations)\n",
                "\n",
                "        # Mask out X chords\n",
                "        mask_no_X = merged_ref != \"X\"\n",
                "        merged_ref = merged_ref[mask_no_X]\n",
                "        merged_est = merged_est[mask_no_X]\n",
                "        durations = durations[mask_no_X]\n",
                "\n",
                "        # Save aggregated data for class-wise metrics.\n",
                "        song_agg_data.append(\n",
                "            {\"merged_ref\": merged_ref, \"merged_est\": merged_est, \"durations\": durations}\n",
                "        )\n",
                "\n",
                "        for e in evals:\n",
                "            comp = e.evaluate(hypotheses=merged_est, references=merged_ref)\n",
                "            score = mir_eval.chord.weighted_accuracy(comp, durations)\n",
                "            song_metric_scores[e].append(score)\n",
                "\n",
                "        # Compute number of transitions in the predicted sequence.\n",
                "        pred_transitions = sum(\n",
                "            1\n",
                "            for j in range(len(pred_labels) - 1)\n",
                "            if pred_labels[j] != pred_labels[j + 1]\n",
                "        )\n",
                "        song_transition_counts.append(pred_transitions)\n",
                "\n",
                "    results = {}\n",
                "    results[\"mean\"] = {}\n",
                "    results[\"median\"] = {}\n",
                "    results[\"std\"] = {}\n",
                "    results[\"boostrap-stde\"] = {}\n",
                "    results[\"bootstrap-95ci\"] = {}\n",
                "    for m in evals:\n",
                "        results[\"mean\"][m.value] = np.mean(song_metric_scores[m])\n",
                "        results[\"median\"][m.value] = np.median(song_metric_scores[m])\n",
                "        results[\"std\"][m.value] = np.std(song_metric_scores[m])\n",
                "        _, se, ci = bootstrap_mean_ci(song_metric_scores[m], num_bootstrap=10000, ci=95)\n",
                "        results[\"boostrap-stde\"][m.value] = se\n",
                "        results[\"bootstrap-95ci\"][m.value] = ci\n",
                "\n",
                "    results[\"avg_transitions_per_song\"] = np.mean(song_transition_counts)\n",
                "\n",
                "    class_agg_results = {}\n",
                "    for e in tqdm(evals, desc=\"Class-wise metrics\"):\n",
                "        # Compute the overall (aggregated) metric using mean and median over chords.\n",
                "        aggregated_class_mean = compute_aggregated_class_metric(\n",
                "            song_agg_data, e, np.mean\n",
                "        )\n",
                "        aggregated_class_median = compute_aggregated_class_metric(\n",
                "            song_agg_data, e, np.median\n",
                "        )\n",
                "        class_agg_results[e.value] = {\n",
                "            \"mean\": aggregated_class_mean,\n",
                "            \"median\": aggregated_class_median,\n",
                "        }\n",
                "    results[\"class_wise\"] = class_agg_results\n",
                "\n",
                "    return results"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Models loaded\n",
                        "Loaded dataset\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "/Users/pierrelardet/.pyenv/versions/UG4Diss/lib/python3.8/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
                        "  return self.fget.__get__(instance, owner)()\n"
                    ]
                }
            ],
            "source": [
                "DIR = f'./results/small_vs_large_vocab'\n",
                "\n",
                "device = get_torch_device()\n",
                "\n",
                "small_exp = 'small'\n",
                "small_model = CRNN(num_classes=26)\n",
                "small_model.load_state_dict(torch.load(f'{DIR}/{small_exp}/best_model.pth', map_location=device, weights_only=True))\n",
                "small_model.eval()\n",
                "\n",
                "big_exp = 'large'\n",
                "big_model = CRNN()\n",
                "big_model.load_state_dict(torch.load(f'{DIR}/{big_exp}/best_model.pth', map_location=device, weights_only=True))\n",
                "\n",
                "print('Models loaded')\n",
                "\n",
                "_, val_filenames, _ = get_split_filenames()\n",
                "val_dataset = FullChordDataset(val_filenames, small_vocab=True, dev_mode=True)\n",
                "\n",
                "print('Loaded dataset')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Evaluating model...\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Predicting: 100%|██████████| 8/8 [00:39<00:00,  4.88s/it]\n",
                        "Evaluating: 100%|██████████| 241/241 [00:46<00:00,  5.20it/s]\n",
                        "Class-wise metrics: 100%|██████████| 6/6 [01:01<00:00, 10.18s/it]\n"
                    ]
                }
            ],
            "source": [
                "small_metrics = evaluate_model_large_vs_small(small_model, is_small=True, dataset=val_dataset)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Evaluating model...\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Predicting: 100%|██████████| 8/8 [00:52<00:00,  6.60s/it]\n",
                        "Evaluating: 100%|██████████| 241/241 [00:28<00:00,  8.33it/s]\n",
                        "Class-wise metrics: 100%|██████████| 6/6 [00:42<00:00,  7.10s/it]\n"
                    ]
                }
            ],
            "source": [
                "big_metrics = evaluate_model_large_vs_small(big_model, is_small=False, dataset=val_dataset)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "{'mean': {'root': 80.16192820434149,\n",
                            "  'mirex': 79.00230595404003,\n",
                            "  'third': 76.69456422683054,\n",
                            "  'seventh': 76.69456422683054,\n",
                            "  'majmin': 76.69456422683054,\n",
                            "  'acc': 76.69456422683054},\n",
                            " 'median': {'root': 83.31648483998393,\n",
                            "  'mirex': 81.433310167812,\n",
                            "  'third': 79.37455087138952,\n",
                            "  'seventh': 79.37455087138952,\n",
                            "  'majmin': 79.37455087138952,\n",
                            "  'acc': 79.37455087138952},\n",
                            " 'std': {'root': 12.542917903212135,\n",
                            "  'mirex': 13.114700775773693,\n",
                            "  'third': 14.876969519385401,\n",
                            "  'seventh': 14.876969519385401,\n",
                            "  'majmin': 14.876969519385401,\n",
                            "  'acc': 14.876969519385401},\n",
                            " 'boostrap-stde': {'root': 0.8031619667156519,\n",
                            "  'mirex': 0.8533863511352406,\n",
                            "  'third': 0.9473519619129844,\n",
                            "  'seventh': 0.9572588406109518,\n",
                            "  'majmin': 0.9551846338964388,\n",
                            "  'acc': 0.9460939939578605},\n",
                            " 'bootstrap-95ci': {'root': (78.52032294438204, 81.67142437076724),\n",
                            "  'mirex': (77.3137583783622, 80.617871971463),\n",
                            "  'third': (74.78449657570361, 78.49904217330506),\n",
                            "  'seventh': (74.77221771452572, 78.52058318445792),\n",
                            "  'majmin': (74.79046352624238, 78.52595174399539),\n",
                            "  'acc': (74.81430781861184, 78.50131848813203)},\n",
                            " 'avg_transitions_per_song': 133.2448132780083,\n",
                            " 'class_wise': {'root': {'mean': 77.75333786168042,\n",
                            "   'median': 78.62873690195562},\n",
                            "  'mirex': {'mean': 75.83338778880763, 'median': 75.54772526336193},\n",
                            "  'third': {'mean': 73.81380414092735, 'median': 73.84228005664576},\n",
                            "  'seventh': {'mean': 73.81380414092735, 'median': 73.84228005664576},\n",
                            "  'majmin': {'mean': 73.81380414092735, 'median': 73.84228005664576},\n",
                            "  'acc': {'mean': 73.81380414092735, 'median': 73.84228005664576}}}"
                        ]
                    },
                    "execution_count": 8,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "small_metrics"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "{'mean': {'root': 79.09213953275189,\n",
                            "  'mirex': 79.18775134956911,\n",
                            "  'third': 76.0492967531169,\n",
                            "  'seventh': 76.0492967531169,\n",
                            "  'majmin': 76.0492967531169,\n",
                            "  'acc': 76.0492967531169},\n",
                            " 'median': {'root': 81.60258834509975,\n",
                            "  'mirex': 81.5537335097427,\n",
                            "  'third': 78.69716067380483,\n",
                            "  'seventh': 78.69716067380483,\n",
                            "  'majmin': 78.69716067380483,\n",
                            "  'acc': 78.69716067380483},\n",
                            " 'std': {'root': 13.144994095227174,\n",
                            "  'mirex': 12.890755898531689,\n",
                            "  'third': 14.750633999945409,\n",
                            "  'seventh': 14.750633999945409,\n",
                            "  'majmin': 14.750633999945409,\n",
                            "  'acc': 14.750633999945409},\n",
                            " 'boostrap-stde': {'root': 0.844519794998979,\n",
                            "  'mirex': 0.8318806724249199,\n",
                            "  'third': 0.9462478501822312,\n",
                            "  'seventh': 0.95020127736229,\n",
                            "  'majmin': 0.9594151543115406,\n",
                            "  'acc': 0.9488888860724315},\n",
                            " 'bootstrap-95ci': {'root': (77.42131911805222, 80.73680403015216),\n",
                            "  'mirex': (77.53164221118799, 80.79368282629916),\n",
                            "  'third': (74.09498418290353, 77.8253978220342),\n",
                            "  'seventh': (74.12908848792135, 77.82635153842561),\n",
                            "  'majmin': (74.12558923833316, 77.91642669412163),\n",
                            "  'acc': (74.11721436578455, 77.85992204680437)},\n",
                            " 'avg_transitions_per_song': 140.4896265560166,\n",
                            " 'class_wise': {'root': {'mean': 76.55731166388716,\n",
                            "   'median': 76.21889770520228},\n",
                            "  'mirex': {'mean': 75.81625962474793, 'median': 76.3375741006504},\n",
                            "  'third': {'mean': 72.69263535206461, 'median': 73.6057540433154},\n",
                            "  'seventh': {'mean': 72.69263535206461, 'median': 73.6057540433154},\n",
                            "  'majmin': {'mean': 72.69263535206461, 'median': 73.6057540433154},\n",
                            "  'acc': {'mean': 72.69263535206461, 'median': 73.6057540433154}}}"
                        ]
                    },
                    "execution_count": 11,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "big_metrics"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "100%|██████████| 1213/1213 [04:22<00:00,  4.62it/s]\n"
                    ]
                }
            ],
            "source": [
                "small_dataset = FullChordDataset(override_small_vocab=True)\n",
                "\n",
                "small_all_preds = []\n",
                "small_all_labels = []\n",
                "\n",
                "with torch.no_grad():  # Use no_grad to speed up inference\n",
                "    for i in tqdm(range(len(small_dataset))):\n",
                "        cqt, label = small_dataset[i]\n",
                "        pred = small_model(cqt.unsqueeze(0))\n",
                "        preds = torch.argmax(pred, dim=2)\n",
                "        small_all_preds.append(preds[0])  # Keep as tensors\n",
                "        small_all_labels.append(label)    # Keep as tensors\n",
                "\n",
                "# Concatenate all predictions and labels at the end\n",
                "small_all_preds = torch.cat(small_all_preds)\n",
                "small_all_labels = torch.cat(small_all_labels)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 22,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "100%|██████████| 1213/1213 [04:34<00:00,  4.41it/s]\n"
                    ]
                }
            ],
            "source": [
                "big_dataset = FullChordDataset()\n",
                "\n",
                "big_all_preds = []\n",
                "\n",
                "with torch.no_grad():  # Use no_grad to speed up inference\n",
                "    for i in tqdm(range(len(big_dataset))):\n",
                "        cqt, label = big_dataset[i]\n",
                "        pred = big_model(cqt.unsqueeze(0))\n",
                "        preds = torch.argmax(pred, dim=2)\n",
                "        big_all_preds.append(preds[0])  # Keep as tensors\n",
                "\n",
                "# Concatenate all predictions and labels at the end\n",
                "big_all_preds = torch.cat(big_all_preds)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 24,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Map large vocabulary predictions to small vocabulary\n",
                "big_all_preds_small_vocab = torch.tensor([large_to_small_vocab_id(id.item()) for id in big_all_preds])"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Small model accuracy on small dataset: 0.81\n",
                        "Big model accuracy on small dataset: 0.78\n"
                    ]
                }
            ],
            "source": [
                "N_mask = small_all_labels != 0 # Mask out N chords\n",
                "\n",
                "# Accuracy of small model on small dataset ignoring N (index 0)\n",
                "small_all_preds_masked = small_all_preds[N_mask]\n",
                "small_all_labels_masked = small_all_labels[N_mask]\n",
                "small_correct = (small_all_preds_masked == small_all_labels_masked).sum().item()\n",
                "small_total = small_all_labels_masked.size(0)\n",
                "small_acc = small_correct / small_total\n",
                "print(f'Small model accuracy on small dataset: {small_acc:.2f}')\n",
                "\n",
                "# Accuracy of big model on small dataset ignoring N (index 0)\n",
                "big_all_preds_masked = big_all_preds_small_vocab[N_mask]\n",
                "big_correct = (big_all_preds_masked == small_all_labels_masked).sum().item()\n",
                "big_total = small_all_labels_masked.size(0)\n",
                "big_acc = big_correct / big_total\n",
                "print(f'Big model accuracy on small dataset: {big_acc:.2f}')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 29,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Small model accuracy on small dataset: 0.77\n",
                        "Big model accuracy on small dataset: 0.73\n"
                    ]
                }
            ],
            "source": [
                "# Accuracy of small model on small dataset without ignoring N\n",
                "small_correct = (small_all_preds == small_all_labels).sum().item()\n",
                "small_total = small_all_labels.size(0)\n",
                "small_acc = small_correct / small_total\n",
                "print(f'Small model accuracy on small dataset: {small_acc:.2f}')\n",
                "\n",
                "# Accuracy of big model on small dataset without ignoring N\n",
                "big_correct = (big_all_preds_small_vocab == small_all_labels).sum().item()\n",
                "big_total = small_all_labels.size(0)\n",
                "big_acc = big_correct / big_total\n",
                "print(f'Big model accuracy on small dataset: {big_acc:.2f}')"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "UG4Diss",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.10"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
